---
layout: default
title: LLM Observability — 製品詳細
---

# LLM Observability — 製品詳細
**日付**: 2026-02-12 | **モデル**: google/gemini-3-pro-preview

### W&B Weave

**概要**: W&B Weaveは、LLMの実験、Eval、Production Monitoringのループを閉じるために設計された開発者中心のツールキットです。Weights & Biasesの堅牢な実験管理インフラを活用し、強力なバージョニング、カスタムLLM-as-a-judgeによるEval、およびカスタムモデルやLoRAとのシームレスな統合を提供します。

**強み**:
- W&Bの成熟した実験管理およびモデルレジストリエコシステムとの深い統合
- Dynamic LeaderboardsやAudio Monitorsを含む高度なEval機能
- カスタムLoRAアダプターやFine-tuning済みモデルのテストをシームレスにサポート
- すべてのアーティファクト（プロンプト、モデル、データセット）に対する堅牢なバージョニング

**弱み**:
- PIIマスキングなどのエンタープライズコンプライアンス機能に関する明示的なドキュメントの不足
- 複雑なエージェントのメモリ状態に関するビジュアルデバッグの重視度が低い
- トークンレベルの分析が、コストやレイテンシのメトリクスに比べて詳細ではない

**最新のアップデート**:
- Audio Monitors: LLM judgeを使用してオーディオ出力（MP3/WAV）を監視・判定するモニターの作成をサポート。(2026-02-01)
- Dynamic Leaderboards: Evalから自動生成されるリーダーボード。永続的なカスタマイズとCSVエクスポートが可能。(2026-01-29)
- PlaygroundでのカスタムLoRA: W&B ArtifactsのカスタムFine-tuning済みLoRAウェイトをWeave Playgroundで直接使用可能に。(2026-01-16)

| カテゴリ | レーティング | 概要 |
|---|---|---|
| Core Observability | ●●● | Weaveは、W&Bエコシステムに深く統合され、入力、出力、ネストされた実行ツリーを自動キャプチャする堅牢なコアTracing機能を提供します。 |
| Agent / RAG Observability | ●●○ | ネストされたスパンによるRAG Evalと一般的なエージェントTracingを強力にサポートしていますが、エージェントメモリや複雑な状態グラフの特定の可視化はそれほど重視されていません。 |
| Evaluation Integration | ●●● | Weaveの際立ったカテゴリであり、Dynamic Leaderboards、広範なLLM-as-a-judge機能（オーディオ含む）、およびデータセットとの緊密な統合を備えています。 |
| Monitoring & Metrics | ●●○ | コスト、レイテンシ、品質メトリクスに焦点を当てた確かなMonitoring機能を提供し、オンラインEval用のカスタムモニターを定義できます。 |
| Experiment / Improvement Loop | ●●● | 成熟したW&Bプラットフォームを活用したバージョニング、実験管理、およびEval結果をモデルのFine-tuningにフィードバックする仕組みにおいて優れています。 |
| DevEx / Integration | ●●● | 多言語SDKや、PlaygroundでカスタムLoRAアダプターを直接テストできる独自の機能など、優れた開発者体験を提供します。 |
| Enterprise & Security | ●○○ | 強力なデプロイオプション（SaaS、専用環境、オンプレミス）を提供していますが、PIIマスキングや監査ログなどの特定のコンプライアンス機能は公開リリースノートに詳細がありません。 |


---

### LangSmith

**概要**: LangSmithは、LLMアプリケーションのための包括的なDevOpsプラットフォームであり、エンドツーエンドのObservability、Eval、およびデプロイ機能を提供します。複雑なエージェントワークフローやRAGパイプラインのTracingに優れ、ツール使用、コスト、レイテンシを詳細に可視化すると同時に、マネージドおよびセルフホストの両方のデプロイモデルをサポートします。

**強み**:
- LangChainおよびLangGraphフレームワークとの深いネイティブ統合
- ヒューマンアノテーションとLLM-as-a-judgeを含む包括的なEvalスイート
- エンタープライズ向けの堅牢なセルフホスティングを含む柔軟なデプロイオプション
- 複雑なエージェントワークフローと階層的なTracingの強力なサポート

**弱み**:
- トレース数に基づく料金モデルは、大規模アプリケーションでは高額になる可能性がある
- 機能セットが複雑なため、単純なユースケースでは過剰に感じられる場合がある
- LangChainとの統合が最も緊密であるため、カスタムスタックではより多くのセットアップが必要になる可能性がある

**最新のアップデート**:
- Client Library v0.7.1: プラットフォーム接続用のPythonおよびJSクライアントライブラリのアップデート。(2026-02-10)
- Customize Trace Previews: UIでのトレースプレビューの表示方法をカスタマイズできる新機能。(2026-02-06)
- Google Gen AI Wrapper: SDKにおけるGoogle Gen AIラッパーのエクスポートとサポート。(2026-01-31)
- Self-Hosted v0.13: セルフホストデプロイオプションの新バージョンリリース。(2026-01-16)

| カテゴリ | レーティング | 概要 |
|---|---|---|
| Core Observability | ●●● | LangSmithは、特に階層的で複雑なエージェントワークフローに最適化された、深いTracing機能を備えた堅牢なコアObservabilityを提供します。 |
| Agent / RAG Observability | ●●● | エージェントおよびRAGアーキテクチャに高度に特化しており、ツール、検索、多段階の推論プロセスを詳細に可視化します。 |
| Evaluation Integration | ●●● | Evalは主要な柱であり、データセット管理、自動化されたLLM-as-a-judgeテスト、ヒューマンアノテーションワークフローのための広範なツールを備えています。 |
| Monitoring & Metrics | ●●● | コスト、レイテンシ、エラー追跡に重点を置いた包括的なMonitoringスイートで、Productionデプロイに適しています。 |
| Experiment / Improvement Loop | ●●● | 強力なプロンプトエンジニアリング、実験管理、データセット管理機能により、緊密なフィードバックループを促進します。 |
| DevEx / Integration | ●●● | 幅広いフレームワークサポート、堅牢なSDK、最新のエンジニアリングワークフローに適合するツールにより、優れた開発者体験を提供します。 |
| Enterprise & Security | ●●● | 強力なセキュリティコンプライアンス、柔軟なデプロイモデル（セルフホスト含む）、および管理コントロールを備えたエンタープライズ対応製品です。 |


---

### Langfuse

**概要**: Langfuseは、Observability、プロンプト管理、Evalを組み合わせたオープンソースのオールインワンLLMエンジニアリングプラットフォームです。エージェントグラフや推論トレースなどの機能で複雑なエージェントワークフローをサポートし、セルフホスティングや監査ログを含む堅牢なエンタープライズ機能を提供します。

**強み**:
- 高いセキュリティが求められる環境に適した、包括的なオープンソースおよびセルフホスト可能なソリューション
- Evalワークフロー（LLM-as-Judge、ヒューマンアノテーション）をトレースと直接強力に統合
- グラフ表示や推論ステップの可視化を含む高度なエージェントObservability機能
- バージョニング、Playground、実験管理を備えた堅牢なプロンプト管理システム

**弱み**:
- 直接的な「トレースの再実行」機能は、トレースビューからのワンクリック操作ではなく、Playgroundを介した手動操作となる
- Fine-tuningのサポートは、トレーニングプロセスの管理ではなく、データセット生成に限定されている
- マルチリージョンクラウドを提供する競合他社と比較して、クラウドリージョンの可用性が明示的に詳細化されていない

**最新のアップデート**:
- バージョン付きデータセットでの実験実行: 特定のタイムスタンプでデータセットを取得し、再現性のために過去のバージョンで実験を実行可能に。(2026-02-11)
- Single Observation Evals: 個別のObservationに直接Evalを追加できる機能。(2026-02-05)
- Thinking / Reasoning Partsのレンダリング: トレース詳細におけるChain-of-thought/推論ステップの可視化。(2026-01-30)
- 組織監査ログビューアー: 組織レベルの監査ログを表示するためのUI。(2026-01-30)
- トレースの修正済み出力: Fine-tuningデータセット構築のため、トレースビューで改善されたLLM出力をキャプチャ。(2026-01-14)

| カテゴリ | レーティング | 概要 |
|---|---|---|
| Core Observability | ●●● | OpenTelemetryをベースにした堅牢なコアTracing機能。実行フロー、コスト、レイテンシを深く可視化し、複雑なデータ専用のビューを提供します。 |
| Agent / RAG Observability | ●●● | エージェントグラフ、ツール使用、内部推論ステップを可視化する特定の機能を備えた、非常に有能なエージェントObservabilityを提供します。 |
| Evaluation Integration | ●●● | 自動化されたLLM judge、ヒューマンアノテーションワークフロー、および実験を通じた体系的な回帰テストをサポートする包括的なEvalスイートです。 |
| Monitoring & Metrics | ●●● | カスタマイズ可能なDashboardと、コスト管理およびトークン使用量に焦点を当てた強力な分析機能を備えています。 |
| Experiment / Improvement Loop | ●●● | 堅牢なプロンプト管理、データセットのバージョニング、モデル改善のための修正キャプチャ機能を備えた、優れた反復ループを提供します。 |
| DevEx / Integration | ●●● | 幅広いSDK/フレームワークサポートと、既存のインフラに適合しやすいAPIファーストのアーキテクチャを備えた開発者フレンドリーな設計です。 |
| Enterprise & Security | ●●● | オープンソースのセルフホスティングオプション、RBAC、および監査ログ機能により、強力なエンタープライズ体制を整えています。 |


---

### Braintrust

**概要**: Braintrustは、複数の言語にわたる強力なSDKサポートを備えた、コードファーストで開発者中心のワークフローを重視する包括的なAI ObservabilityおよびEvalプラットフォームです。Tracing、Eval（LLM-as-a-judge）、データセット管理を緊密に統合し、Playgroundや「Loop」AIアシスタントなどの機能を使用して、チームがプロンプトやモデルを迅速に反復できるようにします。

**強み**:
- 包括的なEvalエコシステム: トレースをデータセット、Playground、自動Scoringにシームレスにリンク
- 優れた開発者体験: 広範なSDK（Go, Ruby, Java, C#）、自動インスツルメンテーション、IDE統合
- 統合されたワークフロー: Productionトレース、プロンプトエンジニアリング、実験の間を即座に移動可能
- 柔軟なデータ分析: 集計やカスタム可視化機能を備えた強力なBTQL/SQLクエリ
- エンタープライズ対応: 強力なセルフホスティングオプション、RBAC、およびセキュリティコントロール

**弱み**:
- ネイティブなFine-tuningなし: エンドツーエンドのMLOpsプラットフォームと比較して、RLHFやモデルFine-tuningのための組み込みオーケストレーションが不足
- ビジュアルビルダーの制限: コードファーストのワークフローに焦点を当てており、非技術ユーザー向けのドラッグ＆ドロップ式プロンプトフロービルダーが不足
- リージョンサポート: 最近のアップデートでは、マルチリージョンのデータレジデンシーが明示的に強調されていない
- メモリ状態の可視化: エージェントTracingは強力だが、エージェントのメモリ状態の特定の可視化は限定的

**最新のアップデート**:
- トレースレベルのScoring: カスタムコードScoringが実行トレース全体にアクセスし、多段階ワークフローやエージェントの動作を評価可能に。(2026-02-01)
- LangSmith統合: TracingおよびEvalの呼び出しをLangSmithとBraintrustの両方に送信、またはBraintrustのみにルーティングするラッパー。(2026-02-01)
- Cursor統合: MCPサーバーを介したCursor IDEとの統合により、エディタ内で直接ログの照会や実験結果の取得が可能に。(2026-02-01)
- カスタムビューでの添付ファイルレンダリング: カスタムトレースビューで画像、動画、音声を直接レンダリングするサポート。(2026-02-01)
- 自動インスツルメンテーション: Python、Ruby、Goアプリケーション向けのコード変更不要なTracingサポート。(2026-01-29)
- Temporal統合: 親子関係マッピングを備えたTemporalワークフローおよびアクティビティの自動Tracing。(2026-01-21)
- TrueFoundry統合: OpenTelemetryを介してTrueFoundry AI GatewayからLLMトレースをエクスポートする統合。(2026-01-21)
- レビュー用カンバンレイアウト: フラグ付きスパンとレビュー状態を管理するための新しいカンバンボードビュー。(2026-01-21)
- トレースページのLoop: 分析とデバッグのために、個別のトレースビューでAIアシスタント「Loop」が直接利用可能に。(2026-01-21)
- 生のトレースデータの表示: 個別のスパンまたはトレースの完全なJSON表現を表示および検索できる機能。(2026-01-21)

| カテゴリ | レーティング | 概要 |
|---|---|---|
| Core Observability | ●●● | 深い階層的Tracingと生データへのアクセスを備えた堅牢なコアObservability。複雑なワークフローのデバッグに最適化されています。 |
| Agent / RAG Observability | ●●○ | ツールTracingと多段階Evalによるエージェントワークフローの強力なサポート。ただし、可視化はグラフベースではなくツリーベースです。 |
| Evaluation Integration | ●●● | トレース、データセット、自動Scoringの間のシームレスなループを提供するEvalのマーケットリーダーです。 |
| Monitoring & Metrics | ●●● | BTQLおよびSQLを介した柔軟なカスタムメトリクスを備えた包括的なMonitoring。 |
| Experiment / Improvement Loop | ●●● | プロンプト、データセット、実験を第一級のバージョニング対象として扱う、改善ループの優れたサポート。 |
| DevEx / Integration | ●●● | 幅広い言語サポート、自動インスツルメンテーション、IDE統合を備えたクラス最高の開発者体験。 |
| Enterprise & Security | ●●○ | セルフホスティングとアクセス制御を備えた強力なエンタープライズ基盤。ただし、特定のリージョンサポートなどの一部のコンプライアンス機能はあまり目立ちません。 |


---

### MLflow

**概要**: MLflowは、従来のMLOpsからフルスタックのGenAIオペレーティングシステムへと拡張された包括的なオープンソースプラットフォームであり、エンドツーエンドのObservability、エージェントEval、およびプロンプトエンジニアリングを備えています。最近のアップデートでは、分散Tracingを使用したエージェントワークフロー、LLM judgeを使用した継続的なオンラインMonitoring、およびAI支援デバッグツールが強調されています。

**強み**:
- ライフサイクル全体（Tracking、Registry、Evals、Observability）をカバーする統合プラットフォーム
- 完全なOpenTelemetry互換性を備えたベンダーニュートラルなアーキテクチャ
- Judge Builder、MemAlign、継続的Monitoringを含む高度なEvalエコシステム
- 広範なフレームワーク統合（LangChain、LlamaIndexなど）とSDKサポート
- ローカルからエアギャップのあるエンタープライズ環境まで対応する柔軟なデプロイオプション

**弱み**:
- セルフホスティングには、SaaS専用ツールと比較して大幅なインフラ管理が必要
- 従来のMLとGenAIの両方をカバーする機能の広さにより、UIの複雑さが高くなる可能性がある
- インタラクティブな「リプレイ」デバッグワークフローは、バッチEvalや比較ほど重視されていない
- マルチワークスペースRBACなどのエンタープライズ機能は、非常に最近の追加（v3.10）である

**最新のアップデート**:
- 組織サポート: 実験やリソースを整理するためのマルチワークスペース環境のサポート。(2026-02-12)
- MLflow Assistant: コンテキストを認識して問題をデバッグおよび修正するための、Claude Codeを搭載した製品内チャットボット。(2026-01-29)
- エージェントパフォーマンスDashboard: レイテンシ、リクエスト数、品質スコア、ツール使用状況を監視するための構築済みチャート。(2026-01-29)
- MemAlign Judge Optimizer: フィードバックからEvalガイドラインを学習し、judgeの精度を向上させるアルゴリズム。(2026-01-29)
- Judge Builder UI: コードなしでカスタムLLM judgeプロンプトを作成、テスト、エクスポートするためのビジュアルインターフェース。(2026-01-29)
- 継続的オンラインMonitoring: リアルタイムの品質評価のために、Productionの着信トレースに対してLLM judgeを自動的に実行。(2026-01-29)
- 分散Tracing: エンドツーエンドの可視化のために、コンテキスト伝播を使用して複数のサービスにわたるリクエストを追跡。(2026-01-29)

| カテゴリ | レーティング | 概要 |
|---|---|---|
| Core Observability | ●●● | 入力、出力、レイテンシを深く可視化し、複雑で分散したエージェントワークフローを処理する、堅牢なOpenTelemetry互換Tracingシステム。 |
| Agent / RAG Observability | ●●● | ツール使用分析、分散コンテキスト伝播、セッションレベルの追跡など、エージェントシステム向けの強力なサポート。 |
| Evaluation Integration | ●●● | ビジュアルJudge Builder、自動化されたjudge最適化（MemAlign）、および継続的なProduction Monitoringを備えた、業界をリードするEval機能。 |
| Monitoring & Metrics | ●●● | 包括的なMonitoring Dashboardにより、手動セットアップなしでエージェントのパフォーマンス、コスト、品質メトリクスをリアルタイムに可視化します。 |
| Experiment / Improvement Loop | ●●● | 統合されたプロンプト管理、継続的Eval、およびモデルバージョニングを通じて、Observabilityと改善ループをシームレスに接続します。 |
| DevEx / Integration | ●●● | 幅広いフレームワークサポート、AI支援デバッグ、およびPythonとTypeScriptの両方の堅牢なSDKによる優れた開発者体験。 |
| Enterprise & Security | ●●● | 新しいマルチワークスペース組織サポートによりエンタープライズ対応していますが、セルフホスティングはSaaSの代替案よりもインフラ管理が必要です。 |


---

### Arize Phoenix

**概要**: Arize Phoenixは、Tracing、データセット、実験を統合ワークフローに統合するオープンソースのAI ObservabilityおよびEvalプラットフォームです。厳格なエンジニアリングループを重視しており、開発者はProductionデータを使用して、個別のトレースのデバッグから体系的なEvalやプロンプトの反復へとシームレスに移行できます。

**強み**:
- 堅牢なセルフホスティングオプション（Docker/Kubernetes）を備えた強力なオープンソース基盤
- Productionトレースを回帰テスト用のEvalデータセットに変換するシームレスなワークフロー
- Playground、バージョニング、スパンリプレイを含む高度なプロンプトエンジニアリングツール
- エージェント専用のメトリクス（ツール選択/呼び出し）を備えた包括的なEvalスイート
- OpenInference標準と広範なSDKサポートによる幅広いエコシステム統合

**弱み**:
- 明示的な組み込みPIIマスキングおよびデータプライバシー変換機能の不足
- エンタープライズコンプライアンスのための監査ログが明示的に詳細化されていない
- モデルバージョニングは、フル機能のモデルレジストリではなく、実験管理に限定されている
- エージェント用のビジュアルワークフロービルダーやDAGエディタは、トレースタイムラインに比べて重視されていない

**最新のアップデート**:
- OpenAI Responses APIタイプサポート: PlaygroundでChat CompletionsとResponses APIタイプの選択をサポート。(2026-02-12)
- データセットEvaluator: データセットにEvaluatorを直接アタッチし、実験中にサーバー側で自動実行可能に。(2026-02-12)
- Playground用カスタムプロバイダー: Playground全体で再利用可能なカスタムAIプロバイダー（OpenAI、Azure、Anthropicなど）の集中設定。(2026-02-11)
- Claude Opus 4.6サポート: 拡張された思考パラメータとコスト追跡を備えたClaude Opus 4.6のPlaygroundサポート。(2026-02-09)
- ツール選択＆呼び出しEvaluator: エージェントのツール選択の正確さとパラメータ呼び出しの正しさを判定する専用Evaluator。(2026-01-31)
- Phoenix CLIコマンド: ターミナルからプロンプト、データセット、実験を管理するための新しいCLIコマンド。(2026-01-22)
- スパンIDを使用したトレースからデータセットへの変換: ソーススパンへの双方向リンクを保持したまま、トレースをデータセットに変換。(2026-01-21)
- トレースとアノテーションのエクスポート: 手動および自動アノテーションと共にトレースをエクスポートするためのCLIサポート。(2026-01-19)
- CLIターミナルアクセス: AIコーディングアシスタントがターミナルコマンドを介してPhoenixデータに直接照会可能に。(2026-01-17)

| カテゴリ | レーティング | 概要 |
|---|---|---|
| Core Observability | ●●● | Phoenixは、OpenTelemetry/OpenInferenceをベースにした堅牢なコアObservabilityを提供し、詳細なトレース可視化、タイムライン分析、デバッグ用のスパンリプレイ機能を備えています。 |
| Agent / RAG Observability | ●●● | ツール使用のための専用Evaluatorとエージェントフレームワークとの深い統合、および標準的なRAG検索Tracingにより、エージェントワークフローを強力にサポートします。 |
| Evaluation Integration | ●●● | プラットフォームの中心的な柱であり、ProductionトレースとEvalデータセットの間の緊密なループを提供し、広範なLLM-as-a-judge機能とヒューマンフィードバックツールによってサポートされています。 |
| Monitoring & Metrics | ●●● | コスト、レイテンシ、品質メトリクスをカバーする包括的なMonitoring Dashboardを提供し、最近では特にエージェントのツール使用パフォーマンスをターゲットにした追加機能も備えています。 |
| Experiment / Improvement Loop | ●●● | 強力なプロンプトエンジニアリングツール、実験管理、データセット管理により、データ駆動型の反復を可能にする堅牢な改善ループを促進します。 |
| DevEx / Integration | ●●● | 幅広いフレームワークサポート、ターミナルワークフロー用の新しいCLI、およびオープンスタンダードに準拠した柔軟なSDKにより、優れた開発者体験を提供します。 |
| Enterprise & Security | ●●○ | エンタープライズデプロイに適した強力なセルフホスティングとアクセス制御機能を備えていますが、PIIマスキングや監査ログなどの特定のコンプライアンス機能は詳細が不明です。 |


---
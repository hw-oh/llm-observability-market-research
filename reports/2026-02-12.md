---
layout: default
title: LLM Observability Market Research - 2026-02-12
---

# Weekly LLM Observability Market Research Report
**Date**: 2026-02-12 | **Model**: google/gemini-3-pro-preview | **Data Collected**: 2026-02-12

## 1. Executive Summary

- LangSmith released v0.13 of its self-hosted platform, reinforcing on-premise capabilities with deep LangGraph integration for visualizing multi-step reasoning.
- MLflow introduced Organization Support for multi-workspace management and a visual Judge Builder to streamline the creation of automated evaluators.
- W&B Weave rolled out dynamic leaderboards and custom scorers, alongside unique audio monitors designed for multi-modal evaluation workflows.
- Braintrust implemented configurable image rendering security controls and expanded agent observability with new Temporal integration for tool call tracing.
- Langfuse launched an Organization Audit Log Viewer to enhance compliance visibility while adding timeline views for debugging complex chain latency.
- Arize Phoenix focused on agentic evaluation by releasing specialized metrics for tool selection accuracy and confirming RBAC support for self-hosted deployments.

**Market Insight by AI**:

> LangSmith and Arize Phoenix are aggressively targeting agentic workflows with specific tool selection evaluators and reasoning traces, pressuring Weave to expand its agent visualization beyond basic tool call tracing.
> While Weave offers strong on-prem options, Langfuse's new Audit Log Viewer and LangSmith's v0.13 self-hosted release highlight a gap in Weave's explicit documentation regarding PII masking and compliance logs.
> Braintrust's deep IDE integration and MLflow's visual Judge Builder challenge Weave's developer-first positioning by offering tighter coupling between the coding environment and evaluation logic.


## 2. Product Feature Comparison

| Product | Trace Depth | Eval | Agent Observability | Cost Tracking | Enterprise Ready | Overall |
|---|---|---|---|---|---|---|
| **W&B Weave** | <details><summary>●●●</summary>Captures nested trace trees and automatically logs inputs, outputs, and latency via decorators.</details> | <details><summary>●●●</summary>Features dynamic leaderboards, custom scorers, and unique audio monitors for multi-modal judging.</details> | <details><summary>●●●</summary>Supports tool call tracing and retrieval evaluation, though memory visualization is less detailed.</details> | <details><summary>●●●</summary>Includes a cost dashboard with customizable views and token usage tracking.</details> | <details><summary>●●○</summary>Offers strong on-prem/VPC options but lacks explicit documentation on PII masking and audit logs.</details> | <details><summary>●●●</summary>A robust platform for ML engineers that excels in linking experimentation and fine-tuning with production monitoring.</details> |
| **LangSmith** | <details><summary>●●●</summary>Provides granular visibility into complex workflows with support for hierarchical spans and streaming.</details> | <details><summary>●●●</summary>Comprehensive suite with pairwise comparisons, human annotation queues, and automated evaluators.</details> | <details><summary>●●●</summary>Deep native integration with LangGraph allows for visualizing multi-step reasoning and tool usage.</details> | <details><summary>●●●</summary>Tracks granular usage and costs across models, tools, and retrieval steps.</details> | <details><summary>●●●</summary>Offers a mature self-hosted version (v0.13), SSO, RBAC, and compliance certifications like SOC 2.</details> | <details><summary>●●●</summary>The definitive platform for the LangChain ecosystem, rapidly expanding with framework-agnostic features and deep enterprise support.</details> |
| **Langfuse** | <details><summary>●●●</summary>Captures detailed nested traces with a timeline view for debugging latency and complex chains.</details> | <details><summary>●●●</summary>Combines automated LLM-as-a-Judge with human annotation queues and dataset management.</details> | <details><summary>●●●</summary>Visualizes agents as graphs and renders reasoning traces for modern thinking models.</details> | <details><summary>●●●</summary>Includes spend alerts and detailed token analytics with support for the latest model pricing.</details> | <details><summary>●●●</summary>Fully self-hostable with new audit log viewers, RBAC, and PII masking capabilities.</details> | <details><summary>●●●</summary>A feature-rich open-source platform that balances strong developer experience with enterprise-grade self-hosting.</details> |
| **Braintrust** | <details><summary>●●●</summary>Logs full execution traces with deep object logging and supports streaming responses.</details> | <details><summary>●●●</summary>Market-leading integration of online scoring, custom code scorers, and regression detection.</details> | <details><summary>●●●</summary>Strong tool call tracing and new Temporal integration, though RAG-specific visualizations are less emphasized.</details> | <details><summary>●●●</summary>Detailed tracking includes cache hit/miss costs and comprehensive token analytics.</details> | <details><summary>●●●</summary>Supports self-hosting and RBAC, with new security controls for image rendering.</details> | <details><summary>●●●</summary>A developer-first platform with exceptional SDK support and IDE integration that unifies evaluation and observability.</details> |
| **MLflow** | <details><summary>●●●</summary>OpenTelemetry-native tracing captures distributed requests and deep introspection of GenAI workflows.</details> | <details><summary>●●●</summary>Features a visual Judge Builder, the MemAlign optimizer, and continuous online evaluation.</details> | <details><summary>●●●</summary>Includes specialized agent performance dashboards and visualization for multi-turn reasoning.</details> | <details><summary>●●●</summary>Tracks evaluation costs and general usage expenses with built-in token counting.</details> | <details><summary>●●●</summary>Introduced Organization Support for multi-workspace environments and includes RBAC.</details> | <details><summary>●●●</summary>The dominant open-source standard offering an all-in-one lifecycle solution from experimentation to deployment.</details> |
| **Arize Phoenix** | <details><summary>●●●</summary>Full OpenTelemetry support with span replay capabilities for debugging specific trace steps.</details> | <details><summary>●●●</summary>Strong focus on dataset evaluators and specialized metrics for tool selection accuracy.</details> | <details><summary>●●●</summary>Excels in evaluating tool usage and reasoning models, supported by specific dashboard widgets.</details> | <details><summary>●●●</summary>Automatic token cost calculation using the latest pricing models.</details> | <details><summary>●●○</summary>Strong self-hosting and RBAC support, but lacks explicit PII masking and audit log features in the provided text.</details> | <details><summary>●●●</summary>A strong open-source choice for developers focusing on rigorous evaluation and dataset curation alongside tracing.</details> |

## 3. New Features (Last 30 Days)

### [W&B Weave](https://app.getbeamer.com/wandb/en)
- **Audio Monitors**: Monitors that observe and judge audio outputs alongside text using LLM judges. (2026-02-01, Monitoring & Metrics)
- **Dynamic Leaderboards**: Auto-generated leaderboards from evaluations with persistent customization and CSV export. (2026-01-29, Evaluation Integration)
- **Custom LoRAs in Playground**: Ability to test and evaluate custom fine-tuned LoRA weights directly in the Weave Playground. (2026-01-16, Experiment / Improvement Loop)

### [LangSmith](https://changelog.langchain.com)
- **Python SDK v0.7.1**: Client library update for connecting to LangSmith Platform. (2026-02-10, DevEx / Integration)
- **Customize trace previews**: Ability to customize how traces are previewed in the UI. (2026-02-06, Core Observability)
- **Google Gen AI Wrapper Export**: Export functionality for Google Gen AI wrapper in SDK. (2026-01-31, DevEx / Integration)
- **LangSmith Self-Hosted v0.13**: New version of the self-hosted platform release. (2026-01-16, Enterprise & Security)

### [Langfuse](https://langfuse.com/changelog)
- **Org Audit Log Viewer**: New UI for viewing organization audit logs. (2026-02-09, Enterprise & Security)
- **Render Thinking/Reasoning Parts**: Support for rendering thinking and reasoning traces (e.g. for reasoning models) in trace details. (2026-02-09, Core Observability)
- **Single Observation Evals**: Ability to add evaluations to single observations. (2026-02-09, Evaluation Integration)
- **Events-based Trace Table**: New events-based view mode for the observation/trace table. (2026-02-09, Core Observability)
- **Corrected Outputs for Traces**: Capture improved versions of LLM outputs directly in trace views to build fine-tuning datasets. (2026-01-14, Experiment / Improvement Loop)

### [Braintrust](https://braintrust.dev/docs/changelog)
- **Render attachments in custom views**: Custom trace views now support rendering images, videos, audio, and other attachments directly. (2026-02, Core Observability)
- **Navigate to trace origins**: Navigate from traces in logs back to their originating prompt or dataset row. (2026-02, Experiment / Improvement Loop)
- **Trace-level scorers**: Custom code scorers can now access the entire execution trace to evaluate multi-step workflows. (2026-02, Evaluation Integration)
- **LangSmith integration**: Wrapper to send tracing and evaluation calls to both LangSmith and Braintrust or route solely to Braintrust. (2026-02, DevEx / Integration)
- **Cursor integration**: Extension to automatically configure Braintrust MCP server for querying logs and experiments from Cursor. (2026-02, DevEx / Integration)
- **Image rendering security controls**: Configurable modes (auto-load, click-to-load, block) to prevent sensitive data leaks from image URLs. (2026-02, Enterprise & Security)
- **Single span filters with aggregations**: Combine single span filters with GROUP BY to aggregate traces based on span-level conditions. (2026-02, Monitoring & Metrics)
- **Auto-instrumentation (Python, Ruby, Go)**: Zero-code tracing support for Python, Ruby, and Go applications. (2026-01, DevEx / Integration)
- **Temporal integration**: Automatically traces Temporal workflows and activities with distributed tracing support. (2026-01, DevEx / Integration)
- **Kanban layout for reviews**: Drag-and-drop interface for managing flagged spans and review status. (2026-01, Evaluation Integration)

### [MLflow](https://mlflow.org/releases)
- **Organization Support**: Support for multi-workspace environments to organize experiments and resources. (2026-02-12, Enterprise & Security)
- **MLflow Assistant**: In-product chatbot powered by Claude Code to help debug agents and fix issues. (2026-01-29, DevEx / Integration)
- **Agent Performance Dashboards**: Pre-built charts for monitoring agent latency, request counts, and quality scores. (2026-01-29, Monitoring & Metrics)
- **MemAlign Judge Optimizer**: Algorithm that learns evaluation guidelines from feedback to improve judge accuracy. (2026-01-29, Evaluation Integration)
- **Judge Builder UI**: Visual interface to create, test, and iterate on LLM judge prompts without code. (2026-01-29, Evaluation Integration)
- **Continuous Online Monitoring**: Automatically run LLM judges on incoming production traces. (2026-01-29, Monitoring & Metrics)
- **Distributed Tracing**: Track requests across multiple services with context propagation. (2026-01-29, Core Observability)

### [Arize Phoenix](https://arize.com/docs/phoenix/release-notes)
- **Dataset Evaluators**: Attach evaluators directly to datasets for automatic server-side execution during experiments. (2026-02-12, Experiment / Improvement Loop)
- **OpenAI Responses API Type Support**: Support for selecting between Chat Completions and Responses API types for OpenAI models. (2026-02-12, DevEx / Integration)
- **Custom Providers for Playground**: Centralized configuration for custom providers to be reused across playground and prompts. (2026-02-11, DevEx / Integration)
- **Claude Opus 4.6 Model Support**: Support for Anthropic's Claude Opus 4.6 with extended thinking parameter support. (2026-02-09, Core Observability)
- **Tool Selection & Invocation Evaluators**: New evaluators to assess agent tool choice accuracy and parameter formatting. (2026-01-31, Agent / RAG Observability)
- **Configurable Email Extraction**: Support for custom email extraction from OAuth2 providers using JMESPath. (2026-01-28, Enterprise & Security)
- **Phoenix CLI Commands**: CLI commands to manage prompts, datasets, and experiments from the terminal. (2026-01-22, DevEx / Integration)
- **Trace to Dataset with Span Associations**: Convert traces to datasets while maintaining bidirectional links to source spans. (2026-01-21, Evaluation Integration)
- **Export Annotations with Traces**: CLI support to export traces alongside their annotations for offline analysis. (2026-01-19, DevEx / Integration)
- **CLI Terminal Access for AI Assistants**: Enables AI coding assistants to query Phoenix data directly via the CLI. (2026-01-17, DevEx / Integration)

## 4. Positioning Shift

| Product | Current | Moving Toward | Signal |
|---|---|---|---|
| W&B Weave | The premier platform for ML engineers building LLM apps who require rigorous experiment tracking and evaluation. | Expanding into multi-modal observability and closing the loop between inference and fine-tuning. | Launch of Audio Monitors and integration of Custom LoRAs into the Playground. |
| LangSmith | The definitive observability and evaluation platform for the LangChain ecosystem, expanding rapidly to support all LLM frameworks. | Becoming the universal operating system for agentic AI, with deeper support for 'Deep Agents', sandboxing, and framework-agnostic workflows. | Recent updates like 'Sandboxes for Deep Agents', 'LangSmith Fetch', and support for Vercel/Pydantic AI indicate a move beyond just tracing into active debugging and broader compatibility. |
| Langfuse | Open-source LLM engineering platform focusing on the complete development lifecycle (observability, prompts, evals). | Deepening agentic observability and enterprise-scale analytics via ClickHouse integration. | Recent updates focusing on reasoning traces, audit logs, and high-performance ClickHouse backend. |
| Braintrust | A developer-first evaluation and observability platform that seamlessly bridges the gap between offline experiments and production monitoring. | Becoming the central operating system for AI engineering by integrating deeply into the developer environment (Cursor, MCP) and supporting complex agentic workflows (Temporal). | Recent releases of Cursor integration, MCP server improvements, and Temporal integration indicate a push into the developer's inner loop and infrastructure. |
| MLflow | The dominant open-source standard for the complete machine learning and GenAI lifecycle. | Becoming the central operating system for Agentic AI with deep observability and automated evaluation. | Release of specialized Agent Observability features, MemAlign Judge Optimizer, and MLflow Assistant. |
| Arize Phoenix | Arize Phoenix is positioned as a developer-first, open-source observability platform that tightly integrates tracing with evaluation and dataset management. | The product is moving toward deeper agentic workflow support and enhanced developer ergonomics through CLI and server-side evaluation capabilities. | Recent releases of Tool Selection Evaluators, Dataset Evaluators, and a comprehensive CLI demonstrate this shift. |

## 5. Enterprise Signals

- LangSmith released v0.13 of its self-hosted platform, reinforcing its commitment to on-premise enterprise deployments.
- MLflow introduced Organization Support to enable multi-workspace environments and better resource management for large teams.
- Langfuse launched a new Organization Audit Log Viewer to enhance compliance and security visibility.
- Braintrust implemented configurable image rendering security controls to prevent data leaks via attachments.
- Arize Phoenix confirmed support for RBAC and LDAP integration alongside its Docker and Kubernetes self-hosting options.

---

## Methodology

Data was collected on 2026-02-12 via Serper.dev web search, official documentation scraping, and GitHub/PyPI feeds.
Analysis was performed using the google/gemini-3-pro-preview model via OpenRouter.


---
layout: default
title: 경쟁사 인텔리전스 리포트 - 2026-02-11
---

# W&B Weave — 경쟁사 인텔리전스 리포트
**날짜**: 2026-02-11 | **모델**: google/gemini-3-pro-preview | **데이터 수집일**: 2026-02-11

---

## 축별 기능 비교

### 트레이싱/옵저버빌리티

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| 자동 트레이싱 및 시각화 | [Weave Tracing](https://weave-docs.wandb.ai/guides/tracking) | [LangSmith Tracing](https://docs.smith.langchain.com/observability) | X | X | X | X | X |
| 비용 및 토큰 추적 | [Token Usage Tracking](https://weave-docs.wandb.ai/guides/tracking) | [Cost & Token Tracking](https://docs.smith.langchain.com/observability#cost-tracking) | X | X | [Token & Cost Tracking](https://langfuse.com/docs/model-usage-and-cost) | X | X |
| AI 기반 디버깅 어시스턴트 | X | [Polly (Beta)](https://changelog.langchain.com) | X | X | X | X | X |
| CLI/터미널 트레이스 디버깅 | X | [LangSmith Fetch](https://changelog.langchain.com) | X | X | X | X | X |
| 중첩 스팬(Nested Spans) UI | [Trace UI](https://weave-docs.wandb.ai/guides/tracking) | [Trace View](https://docs.smith.langchain.com/observability) | X | X | X | X | X |
| 자동 트레이싱 (Auto-instrumentation) | [Weave Auto-patching](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | [Phoenix Auto-instrumentation (OpenInference)](https://docs.arize.com/phoenix/tracing/how-to-tracing) | X | X | X | X |
| 트레이스 시각화 UI | [Weave Traces UI](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | [Phoenix Trace UI](https://docs.arize.com/phoenix/tracing/overview-tracing) | X | X | X | X |
| 스팬 리플레이 (디버깅) | X | X | [Span Replay](https://docs.arize.com/phoenix/prompt-engineering/span-replay) | X | X | X | X |
| OpenTelemetry 표준 지원 | X | X | [OTLP Tracing Support](https://docs.arize.com/phoenix/tracing/how-to-tracing/opentelemetry) | X | X | X | X |
| 비동기 트레이싱 | [Async Logging](https://weave-docs.wandb.ai/guides/tracking) | X | [Async Tracing](https://docs.arize.com/phoenix) | X | X | X | X |
| 자연어 로그 쿼리/분석 | X | X | X | [Loop](https://braintrust.dev/docs/start/observe) | X | X | X |
| 자동 트레이싱/인스트루먼테이션 | [Weave Auto-patching](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | [Auto-instrumentation (Python, TS, Go, Ruby)](https://braintrust.dev/docs/start/instrument) | X | X | X |
| 멀티턴 대화 뷰 | [Trace UI](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | [Thread View](https://braintrust.dev/docs/changelog#january-2026) | X | X | X |
| 리뷰 관리 UI | X | X | X | [Kanban layout for reviews](https://braintrust.dev/docs/changelog#january-2026) | X | X | X |
| 비용/토큰 추적 | [Cost & Usage Tracking](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | [Token & Cost Tracking](https://braintrust.dev/docs/start/observe) | X | X | X |
| 자동 트레이싱 및 스팬 | [Weave Tracing](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | [Langfuse Tracing (OpenTelemetry based)](https://langfuse.com/docs/observability/overview) | X | X |
| 세션/스레드 뷰 | [Weave Threads/Calls](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | [Sessions](https://langfuse.com/docs/tracing/sessions) | X | X |
| 에이전트 시각화 | Trace UI | X | X | X | Agent Graphs | X | X |
| 사용자 식별 | [Attributes/Metadata](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | [User Tracking](https://langfuse.com/docs/tracing/users) | X | X |
| LLM 호출 로깅 | [Weave Tracing](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | X | [Logs / Observability](https://humanloop.com/docs/observability) | X |
| 사용자 피드백 수집 | [Feedback UI](https://weave-docs.wandb.ai/guides/tracking/feedback) | X | X | X | X | [User Feedback / Human Review](https://humanloop.com/docs/observability/capture-feedback) | X |
| 비용 및 지연 시간 추적 | [Cost & Latency Tracking](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | X | [Performance Monitoring](https://humanloop.com/docs/observability) | X |
| 중첩 스팬(Nested Spans) 시각화 | [Trace Timeline UI](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | X | [Traces](https://humanloop.com/docs/observability) | X |
| 자동 트레이싱 및 계측 | [Weave Tracing (Auto-patching)](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | X | X | logfire.instrument_* / Auto-tracing |
| 트레이스 데이터 쿼리 | X | X | X | X | X | X | SQL Queries for Traces |
| 중첩 스팬 UI | [Trace Timeline](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | X | X | [Live View / Flame Graphs](https://logfire.pydantic.dev/docs/) |
| 비동기 지원 | Async Tracing | X | X | X | X | X | [Async Support (asyncio)](https://logfire.pydantic.dev/docs/) |
| Pydantic 모델 통합 | Object Versioning | X | X | X | X | X | [Pydantic Integration](https://logfire.pydantic.dev/docs/integrations/pydantic/) |

### 평가 파이프라인

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| 데이터셋 기반 평가 실행 | [weave.Evaluation](https://weave-docs.wandb.ai/guides/evaluation) | [LangSmith Evaluation](https://docs.smith.langchain.com/evaluation) | X | X | [Experiments](https://langfuse.com/docs/evaluation/experiments) | [Evaluations](https://humanloop.com/docs/evaluation) | X |
| 쌍대 비교(Pairwise Comparison) | [Evaluation Comparison UI](https://weave-docs.wandb.ai/guides/evaluation) | [Pairwise Annotation Queues](https://docs.smith.langchain.com/evaluation/faq/pairwise-comparison) | X | X | X | X | X |
| 온라인/프로덕션 평가 | X | [Online Evaluation / Monitoring](https://docs.smith.langchain.com/observability#online-evaluation) | X | X | X | X | X |
| 사람에 의한 피드백/어노테이션 | X | [Annotation Queues](https://docs.smith.langchain.com/evaluation/how_to/annotation) | X | X | X | X | X |
| CI/CD 파이프라인 통합 | [CI/CD Integration](https://weave-docs.wandb.ai/guides/evaluation) | [Automated Evaluation](https://docs.smith.langchain.com/evaluation) | X | X | X | X | X |
| 평가 실행 및 비교 | [weave.Evaluation](https://weave-docs.wandb.ai/guides/evaluation) | X | [Phoenix Experiments](https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets-and-experiments) | X | X | X | X |
| 데이터셋 기반 평가 | [Evaluation on Datasets](https://weave-docs.wandb.ai/guides/evaluation) | X | [Datasets & Experiments](https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments) | X | X | X | X |
| 외부 평가 라이브러리 통합 | [Custom Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | Evaluator Integrations (Ragas, Deepeval) | X | X | X | X |
| 코드 기반 검증 | [Python Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | [Code-based checks](https://docs.arize.com/phoenix/evaluation/overview-evals) | X | X | X | X |
| 인간 피드백/라벨링 | [Human Feedback / Annotations](https://weave-docs.wandb.ai/guides/tracking/feedback) | X | [Human annotations](https://docs.arize.com/phoenix/evaluation/overview-evals) | X | X | X | X |
| 평가 실행 프레임워크 | [weave.Evaluation](https://weave-docs.wandb.ai/guides/evaluation/weave_eval) | X | X | [braintrust.Eval](https://braintrust.dev/docs/start/evaluate) | X | X | X |
| 실험 비교 UI | [Evaluation Comparison](https://weave-docs.wandb.ai/guides/evaluation/weave_eval) | X | X | [Experiments Comparison](https://braintrust.dev/docs/start/evaluate) | X | X | X |
| 프롬프트 반복 개선 | Playground | X | X | [Playground Iteration](https://braintrust.dev/docs/start/evaluate) | X | X | X |
| 합격/불합격 임계값 | Scorer Thresholds (Custom) | X | X | [Pass/fail thresholds](https://braintrust.dev/docs/changelog#december-2025) | X | X | X |
| 원격 평가 실행 | X | X | X | [Remote Evaluations](https://braintrust.dev/docs/start/evaluate) | X | X | X |
| LLM 기반 자동 평가 | [Weave Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | [LLM-as-a-judge](https://langfuse.com/docs/scores/model-based-evals) | X | X |
| 휴먼 라벨링 워크플로우 | [Human Feedback UI](https://weave-docs.wandb.ai/guides/evaluation/human-feedback) | X | X | X | Annotation Queues | X | X |
| 비교 뷰 | [Evaluation Comparison](https://weave-docs.wandb.ai/guides/evaluation/weave-eval) | X | X | X | Experiment Compare View | X | X |
| 사용자 피드백 수집 | Feedback API | X | X | X | [User Feedback (Browser SDK)](https://langfuse.com/docs/scores/user-feedback) | X | X |
| 평가 결과 비교 | [Evaluation Comparison UI](https://weave-docs.wandb.ai/guides/evaluation) | X | X | X | X | [Compare Mode](https://humanloop.com/docs/evaluation) | X |
| CI/CD 통합 | [Programmatic Evaluation](https://weave-docs.wandb.ai/guides/evaluation) | X | X | X | X | [CI/CD Integration](https://humanloop.com/docs/evaluation/ci-cd) | X |
| 온라인(프로덕션) 평가 | [Production Monitoring](https://weave-docs.wandb.ai/guides/tracking) | X | X | X | X | [Online Evaluations](https://humanloop.com/docs/evaluation/online) | X |
| 코드 기반 평가 정의 | [weave.Evaluation class](https://weave-docs.wandb.ai/guides/evaluation) | X | X | X | X | [Code-first Evals](https://humanloop.com/docs/evaluation/code) | X |
| 평가 객체/파이프라인 | [weave.Evaluation](https://weave-docs.wandb.ai/guides/evaluation/evaluation-pipeline) | X | X | X | X | X | X |
| 평가 비교 뷰 | [Evaluation Comparison UI](https://weave-docs.wandb.ai/guides/evaluation/evaluation-pipeline) | X | X | X | X | X | X |
| 프로그래매틱 평가 실행 | [weave.Evaluation.evaluate()](https://weave-docs.wandb.ai/guides/evaluation/evaluation-pipeline) | X | X | X | X | X | X |
| 피드백/어노테이션 기록 | [Feedback UI](https://weave-docs.wandb.ai/guides/tracking/feedback) | X | X | X | X | X | [Feedback Annotations (Experimental)](https://logfire.pydantic.dev/docs/) |
| 데이터셋 기반 실행 | [Evaluation on Dataset](https://weave-docs.wandb.ai/guides/evaluation/evaluation-pipeline) | X | X | X | X | X | X |

### 데이터셋 관리

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| 데이터셋 객체 및 버전 관리 | [weave.Dataset](https://weave-docs.wandb.ai/guides/datasets) | [Datasets & Testing](https://docs.smith.langchain.com/evaluation) | X | [Datasets](https://braintrust.dev/docs/start/annotate) | X | X | X |
| 트레이스에서 데이터셋 생성 | [Create Dataset from Traces](https://weave-docs.wandb.ai/guides/datasets) | [Add to Dataset from Trace](https://docs.smith.langchain.com/observability) | [Create Datasets from Traces](https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets) | X | [Add to Dataset from Trace](https://langfuse.com/docs/datasets/overview) | X | X |
| UI 내 데이터 편집 | [Dataset UI](https://weave-docs.wandb.ai/guides/datasets) | [Dataset UI](https://docs.smith.langchain.com/evaluation) | X | [Dataset UI Editor](https://braintrust.dev/docs/start/annotate) | X | [Dataset Editor](https://humanloop.com/docs/data) | X |
| SDK를 통한 데이터 관리 | [Weave SDK](https://weave-docs.wandb.ai/guides/datasets) | [LangSmith SDK](https://docs.smith.langchain.com/reference/sdk_reference) | X | X | X | X | X |
| 다양한 포맷(CSV/JSON) 지원 | [CSV/Pandas Integration](https://weave-docs.wandb.ai/guides/datasets) | [Import/Export](https://docs.smith.langchain.com/evaluation) | X | X | X | X | X |
| 데이터셋 객체 관리 | [weave.Dataset](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | [Phoenix Datasets](https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets) | X | X | X | X |
| 데이터셋 버전 관리 | [W&B Artifact Versioning](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | [Dataset Versioning](https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets-and-experiments) | X | [Dataset Versioning](https://langfuse.com/docs/datasets/overview) | [Datasets](https://humanloop.com/docs/data) | X |
| 데이터셋 내보내기 (Fine-tuning) | [Export to W&B Artifacts](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | [Export for fine-tuning](https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets-and-experiments) | X | X | X | X |
| UI 내 데이터셋 편집 | [Weave Dataset UI](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | [Dataset UI](https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets) | X | X | X | X |
| 인라인 데이터셋 생성 | SDK Creation | X | X | [Inline dataset creation](https://braintrust.dev/docs/changelog#february-2026) | X | X | X |
| 데이터셋 스키마 정의 | X | X | X | [Dataset Schemas](https://braintrust.dev/docs/changelog#november-2025) | X | X | X |
| 외부 데이터 임포트 | Pandas/List Import | X | X | [CSV/SQL Import](https://braintrust.dev/docs/start/annotate) | X | X | X |
| 데이터 스키마 검증 | X | X | X | X | JSON Schema Enforcement | X | X |
| 데이터셋 편집 UI | Weave UI Editing | X | X | X | [Dataset UI](https://langfuse.com/docs/datasets/overview) | X | X |
| SDK 데이터셋 접근 | [weave.Dataset](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | X | X | [Dataset SDK](https://langfuse.com/docs/datasets/overview) | X | X |
| 로그에서 데이터셋 생성 | [Create Dataset from Traces](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | X | X | X | [Add to Dataset from Logs](https://humanloop.com/docs/data) | X |
| SDK 데이터셋 관리 | [weave.Dataset SDK](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | X | X | X | [Dataset SDK](https://humanloop.com/docs/data) | X |
| 바이너리/이미지 데이터 지원 | [Multimodal Dataset Support](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | X | X | X | X | X |
| 데이터셋 객체 | [weave.Dataset](https://weave-docs.wandb.ai/guides/core-types/datasets) | X | X | X | X | X | X |
| SDK 데이터셋 업로드 | weave.publish | X | X | X | X | X | X |
| 골든 데이터셋 관리 | Dataset Management | X | X | X | X | X | X |

### 프롬프트 관리

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| 프롬프트 버전 관리 | [weave.Prompt Versioning](https://weave-docs.wandb.ai/guides/prompts) | [Prompt Hub / Versioning](https://docs.smith.langchain.com/prompt_engineering) | [Prompt Management (Versioning)](https://docs.arize.com/phoenix/prompt-engineering/overview-prompts) | [Prompt Management](https://braintrust.dev/docs/start/evaluate) | Prompt Versioning | [Prompt Management / Versions](https://humanloop.com/docs/prompt-management) | X |
| 프롬프트 플레이그라운드 | [Prompt Playground](https://weave-docs.wandb.ai/guides/prompts) | [Playground](https://docs.smith.langchain.com/prompt_engineering) | [Prompt Playground](https://docs.arize.com/phoenix/prompt-engineering/overview-prompts) | X | [LLM Playground](https://langfuse.com/docs/playground) | [Editor / Playground](https://humanloop.com/docs/prompt-management/editor) | X |
| 커뮤니티 프롬프트 허브 | X | [LangChain Hub](https://smith.langchain.com/hub) | X | X | X | X | X |
| 구조화된 프롬프트 객체 | [weave.StringPrompt / MessagesPrompt](https://weave-docs.wandb.ai/guides/prompts) | [LangChain Prompt Templates](https://docs.smith.langchain.com/prompt_engineering) | X | X | X | X | X |
| 프롬프트 A/B 테스팅 | [Evaluation Comparison](https://weave-docs.wandb.ai/guides/evaluation) | [Prompt Testing](https://docs.smith.langchain.com/prompt_engineering) | X | X | X | X | X |
| 코드-프롬프트 동기화 | [Prompt Object Publishing](https://weave-docs.wandb.ai/guides/core-types/prompts) | X | [Prompts in Code (SDK Sync)](https://docs.arize.com/phoenix/prompt-engineering/overview-prompts) | X | X | X | X |
| 프롬프트 템플릿 지원 | [Prompt Templating](https://weave-docs.wandb.ai/guides/core-types/prompts) | X | [Prompt Templates](https://docs.arize.com/phoenix/prompt-engineering/how-to-prompts) | X | X | X | X |
| 모델 간 비교 | [Evaluation Comparison](https://weave-docs.wandb.ai/guides/evaluation) | X | [Compare models side-by-side](https://docs.arize.com/phoenix/prompt-engineering/overview-prompts) | X | X | X | X |
| 고급 템플릿 문법 | X | X | X | [Nunjucks templating](https://braintrust.dev/docs/changelog#december-2025) | X | X | X |
| 플레이그라운드 통합 | Playground | X | X | [Playgrounds](https://braintrust.dev/docs/start/evaluate) | X | X | X |
| 프롬프트 배포 | Artifact Usage | X | X | [Deploy Prompts (AI Proxy)](https://braintrust.dev/docs/start/deploy) | X | X | X |
| MCP 서버 연동 | X | X | X | [MCP servers in prompts](https://braintrust.dev/docs/changelog#december-2025) | X | X | X |
| 배포 및 라벨링 | X | X | X | X | Prompt Deployment (Labels) | X | X |
| 프롬프트 가져오기 SDK | [weave.ref().get()](https://weave-docs.wandb.ai/guides/core-types/prompts) | X | X | X | [get_prompt() SDK](https://langfuse.com/docs/prompts/get-started) | X | X |
| MCP 서버 지원 | X | X | X | X | MCP Server for Prompts | X | X |
| 코드-UI 동기화 | [Code-first Object Management](https://weave-docs.wandb.ai/guides/core-types/models) | X | X | X | X | [.prompt files](https://humanloop.com/docs/prompt-management/prompt-files) | X |
| 프롬프트 템플릿 배포 | X | X | X | X | X | [Deployments / Environments](https://humanloop.com/docs/prompt-management/deployments) | X |
| A/B 테스트 지원 | X | X | X | X | X | [A/B Testing](https://humanloop.com/docs/evaluation/ab-testing) | X |
| 프롬프트 객체 버전 관리 | [weave.StringPrompt / MessagesPrompt](https://weave-docs.wandb.ai/guides/core-types/prompts) | X | X | X | X | X | X |
| 프롬프트 템플릿 편집 | Prompt Editor | X | X | X | X | X | X |
| 프롬프트 배포/참조 | weave.ref() | X | X | X | X | X | X |
| CLI 프롬프트 디버깅 | X | X | X | X | X | X | [logfire prompt (CLI)](https://logfire.pydantic.dev/docs/) |

### 스코어링

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| LLM-as-a-Judge | [Model-based Scorers](https://weave-docs.wandb.ai/guides/evaluation) | [LLM Evaluators](https://docs.smith.langchain.com/evaluation/evaluator-implementations) | X | [LLM Scorers](https://braintrust.dev/docs/start/evaluate) | X | X | X |
| 커스텀 Python 스코어러 | [Custom Scorers](https://weave-docs.wandb.ai/guides/evaluation) | [Custom Evaluators](https://docs.smith.langchain.com/evaluation) | [Code-based checks](https://docs.arize.com/phoenix/evaluation/overview-evals) | X | X | [Code Evaluators](https://humanloop.com/docs/evaluation/evaluators) | Custom Spans/Metrics |
| 내장 스코어러 (환각 등) | [Built-in Scorers](https://weave-docs.wandb.ai/guides/evaluation) | [Off-the-shelf Evaluators](https://docs.smith.langchain.com/evaluation) | X | X | X | X | X |
| 휴먼 피드백/어노테이션 | X | [Annotation Queues](https://docs.smith.langchain.com/evaluation/how_to/annotation) | X | X | X | X | X |
| 사용자 피드백 수집 (Thumbs up/down) | [Feedback UI](https://weave-docs.wandb.ai/guides/tracking) | [Feedback API](https://docs.smith.langchain.com/observability) | X | X | X | X | X |
| LLM 기반 스코어러 | [LLM-as-a-judge Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | [LLM-based evaluators](https://docs.arize.com/phoenix/evaluation/overview-evals) | X | X | X | X |
| 환각/충실도 평가 | [Hallucination Scorer](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | [FaithfulnessEvaluator / HallucinationEvaluator](https://github.com/Arize-ai/phoenix/releases/tag/arize-phoenix-evals-v2.9.0) | X | X | X | X |
| 도구 사용 평가 | [Function Calling Scorer](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | [Tool invocation accuracy metric](https://github.com/Arize-ai/phoenix/releases/tag/arize-phoenix-evals-v2.9.0) | X | X | X | X |
| RAG 평가 메트릭 | [RAG Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | [RAG Analysis / Retrieval Evals](https://docs.arize.com/phoenix/evaluation/pre-built-evals) | X | X | X | X |
| 프로덕션 자동 스코어링 | X | X | X | [Online Scoring Rules](https://braintrust.dev/docs/changelog#january-2026) | X | X | X |
| 전체 트레이스 기반 평가 | [Scorer (Model-based)](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | [Trace-level scorers](https://braintrust.dev/docs/changelog#february-2026) | X | X | X |
| 사용자 피드백 수집 | Feedback UI | X | X | [User Feedback / Annotate](https://braintrust.dev/docs/start/annotate) | X | X | X |
| 스코어링 규칙 생성 UI | X | X | X | [Create scoring rules UI](https://braintrust.dev/docs/changelog#january-2026) | X | X | X |
| LLM 기반 스코어링 | [Weave Scorers](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | [Model-based Evals](https://langfuse.com/docs/scores/model-based-evals) | X | X |
| 커스텀 스코어 함수 | [Custom Scorer Functions](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | [Custom Scores (API/SDK)](https://langfuse.com/docs/scores/custom) | X | X |
| 수동 평가 UI | [Human Feedback](https://weave-docs.wandb.ai/guides/evaluation/human-feedback) | X | X | X | [Manual Scoring (UI)](https://langfuse.com/docs/scores/manually) | X | X |
| 스코어 분석 대시보드 | Evaluation Dashboard | X | X | X | Score Analytics | X | X |
| 주석 큐(Annotation Queue) | X | X | X | X | Annotation Queues | X | X |
| LLM 기반 자동 평가 | [LLM-as-a-judge Scorer](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | X | [LLM Evaluators](https://humanloop.com/docs/evaluation/evaluators) | X |
| 사용자 피드백 (Thumbs up/down) | [Human Feedback](https://weave-docs.wandb.ai/guides/tracking/feedback) | X | X | X | X | [Feedback](https://humanloop.com/docs/observability/capture-feedback) | X |
| 환각(Hallucination) 감지 | [Hallucination Scorer](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | X | [Factuality Evaluators](https://humanloop.com/docs/evaluation/evaluators) | X |
| RAG 검색 품질 평가 | [RAG Evaluation (RAGAS etc.)](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | X | [RAG Evaluators](https://humanloop.com/docs/tutorials/evaluate-rag) | X |
| LLM-as-a-Judge 지원 | [LLM Scorer Class](https://weave-docs.wandb.ai/guides/evaluation/scorers) | X | X | X | X | X | X |
| 이진/수치 스코어링 | Scorer Output Types | X | X | X | X | X | Span Attributes |
| 모델 기반 평가 | Model-based Evaluation | X | X | X | X | X | X |

### LLM/프레임워크 통합

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| LangChain/LangGraph 자동 통합 | [LangChain Integration](https://weave-docs.wandb.ai/guides/integrations) | [LangChain Integration](https://docs.smith.langchain.com/integrations) | X | X | X | X | X |
| 주요 LLM 제공자 (OpenAI 등) 지원 | [OpenAI/Anthropic Integration](https://weave-docs.wandb.ai/guides/integrations) | [Provider Integrations](https://docs.smith.langchain.com/integrations) | X | X | X | X | X |
| 타사 에이전트 프레임워크 지원 | [CrewAI / DSPy / LlamaIndex](https://weave-docs.wandb.ai/guides/integrations) | [CrewAI / AutoGen Integration](https://docs.smith.langchain.com/integrations) | X | X | X | X | X |
| TypeScript/JS SDK 지원 | X | [LangSmith JS SDK](https://docs.smith.langchain.com/reference/sdk_reference) | X | X | X | X | X |
| 프레임워크 비종속적 트레이싱 | [weave.trace Decorator](https://weave-docs.wandb.ai/guides/tracking) | [Traceable Decorator](https://docs.smith.langchain.com/observability) | X | X | X | X | X |
| LangChain 통합 | [LangChain Integration](https://weave-docs.wandb.ai/guides/integrations/langchain) | X | [LangChain Instrumentation](https://docs.arize.com/phoenix/integrations/langchain) | X | X | [LangChain Integration](https://humanloop.com/docs/integrations/langchain) | X |
| LlamaIndex 통합 | [LlamaIndex Integration](https://weave-docs.wandb.ai/guides/integrations/llamaindex) | X | LlamaIndex Instrumentation | X | X | X | X |
| DSPy 통합 | [DSPy Integration](https://weave-docs.wandb.ai/guides/integrations/dspy) | X | [DSPy Instrumentation](https://docs.arize.com/phoenix/integrations/dspy) | X | X | X | instrument_dspy (v4.19.0) |
| OpenAI 통합 | [OpenAI Integration](https://weave-docs.wandb.ai/guides/integrations/openai) | X | [OpenAI Instrumentation](https://docs.arize.com/phoenix/integrations/openai) | X | X | X | X |
| Vercel AI SDK 통합 | X | X | Vercel AI SDK Instrumentation | X | X | [Vercel AI SDK](https://humanloop.com/docs/integrations/vercel-ai-sdk) | X |
| 워크플로우 엔진 통합 | X | X | X | [Temporal Integration](https://braintrust.dev/docs/changelog#january-2026) | X | X | X |
| IDE 통합 | X | X | X | [Cursor Integration](https://braintrust.dev/docs/changelog#february-2026) | X | X | X |
| 경쟁사 호환성 | X | X | X | [LangSmith Wrapper](https://braintrust.dev/docs/changelog#february-2026) | X | X | X |
| 주요 프레임워크 자동 패칭 | [Auto-patching](https://weave-docs.wandb.ai/guides/integrations) | X | X | [Auto-instrumentation (LangChain, etc.)](https://braintrust.dev/docs/start/instrument) | X | X | X |
| 에이전트 코딩 툴 통합 | X | X | X | [Claude Code Integration](https://braintrust.dev/docs/changelog#december-2025) | X | X | X |
| 주요 LLM 프레임워크 통합 | [Integrations](https://weave-docs.wandb.ai/guides/integrations) | X | X | X | [Integrations (LangChain, LlamaIndex, etc.)](https://langfuse.com/docs/integrations/overview) | X | X |
| OpenAI SDK 자동 패칭 | [OpenAI Autologging](https://weave-docs.wandb.ai/guides/integrations/openai) | X | X | X | [OpenAI Integration](https://langfuse.com/docs/integrations/openai) | X | X |
| 데코레이터 기반 추적 | [@weave.op()](https://weave-docs.wandb.ai/guides/tracking/tracing) | X | X | X | [@observe() decorator](https://langfuse.com/docs/sdk/python/decorators) | X | X |
| OpenTelemetry 지원 | X | X | X | X | OpenTelemetry Native | X | X |
| JS/TS SDK 지원 | X | X | X | X | [JS/TS SDK](https://langfuse.com/docs/sdk/typescript/guide) | X | X |
| 주요 LLM API 지원 | [Model Integrations](https://weave-docs.wandb.ai/guides/integrations) | X | X | X | X | [Supported Models](https://humanloop.com/docs/reference/supported-models) | X |
| OpenAI/Anthropic 통합 | [OpenAI/Anthropic Autolog](https://weave-docs.wandb.ai/guides/integrations/) | X | X | X | X | X | instrument_openai / instrument_anthropic |
| LangChain/LlamaIndex 통합 | [LangChain/LlamaIndex Integration](https://weave-docs.wandb.ai/guides/integrations/) | X | X | X | X | X | instrument_langchain / instrument_llamaindex |
| PydanticAI 통합 | X | X | X | X | X | X | instrument_pydantic_ai |
| LiteLLM 통합 | [LiteLLM Integration](https://weave-docs.wandb.ai/guides/integrations/litellm) | X | X | X | X | X | instrument_litellm |

### 가격

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| 무료 티어 제공 | [W&B Free Tier](https://wandb.ai/site/pricing) | [Developer Plan (Free)](https://www.langchain.com/pricing) | X | X | X | X | [Free Tier (10M spans/month)](https://pydantic.dev/pricing) |
| 사용량 기반 과금 (트레이스 수) | [Usage-based Pricing](https://wandb.ai/site/pricing) | [Trace-based Pricing](https://www.langchain.com/pricing) | X | X | X | X | X |
| 팀 단위 정액제 | [W&B Teams](https://wandb.ai/site/pricing) | [Plus Plan ($39/seat)](https://www.langchain.com/pricing) | X | X | X | X | X |
| 엔터프라이즈 커스텀 | [W&B Enterprise](https://wandb.ai/site/pricing) | [Enterprise Plan](https://www.langchain.com/pricing) | X | X | X | X | X |
| 데이터 보존 기간 제한 | [Retention Policy](https://wandb.ai/site/pricing) | [Retention Policy (14-400 days)](https://www.langchain.com/pricing) | X | X | X | X | X |
| 무료 티어 | [W&B Free Tier](https://wandb.ai/site/pricing) | X | [Free (Open Source & SaaS Individual)](https://arize.com/pricing/) | [Free Tier](https://www.braintrust.dev/pricing) | [Hobby Plan (Free)](https://langfuse.com/pricing) | [Starter Plan (Historical)](https://humanloop.com/pricing) | X |
| 사용량 기반 과금 | [Teams / Enterprise](https://wandb.ai/site/pricing) | X | [Pro Plan / Enterprise](https://arize.com/pricing/) | [Usage-based scaling](https://www.braintrust.dev/pricing) | [Usage-based (Units)](https://langfuse.com/pricing) | [Usage-based (Historical)](https://humanloop.com/pricing) | [$2 per million logs/spans](https://pydantic.dev/pricing) |
| 오픈소스 무료 사용 | X | X | [Open Source (Unlimited Local)](https://github.com/Arize-ai/phoenix) | X | [OSS Self-host (Free)](https://langfuse.com/pricing-self-host) | X | X |
| 엔터프라이즈 플랜 | [Enterprise](https://wandb.ai/site/pricing) | X | [Enterprise Plan](https://arize.com/pricing/) | [Enterprise](https://www.braintrust.dev/pricing) | [Enterprise](https://langfuse.com/enterprise) | X | X |
| 팀 협업 기능 | [Teams](https://wandb.ai/site/pricing) | X | [Teams Plan](https://arize.com/pricing/) | X | [Pro Plan](https://langfuse.com/pricing) | X | X |
| 팀/프로 플랜 | [Teams Plan (Usage based)](https://wandb.ai/site/pricing) | X | X | [Pro Plan ($249/mo)](https://www.braintrust.dev/pricing) | X | X | X |
| 좌석(Seat) 기반 과금 | [Per seat pricing](https://wandb.ai/site/pricing) | X | X | [Per seat pricing (Pro)](https://www.braintrust.dev/pricing) | X | [Per Seat Pricing (Historical)](https://humanloop.com/pricing) | [Included in Plans](https://pydantic.dev/pricing) |
| 엔터프라이즈 요금제 | [Enterprise Plan](https://wandb.ai/site/pricing) | X | X | X | X | [Enterprise Plan (Historical)](https://humanloop.com/pricing) | X |
| 신규 가입 가능 여부 | [Available](https://wandb.ai/site/pricing) | X | X | X | X | X | X |
| 팀/엔터프라이즈 플랜 | [Teams / Enterprise](https://wandb.ai/site/pricing) | X | X | X | X | X | [Team ($49) / Growth ($249)](https://pydantic.dev/pricing) |
| 보존 기간 | Plan dependent | X | X | X | X | X | [Plan dependent](https://pydantic.dev/pricing) |

### 셀프호스팅

| 기능 | **Weave** | LangSmith | Arize Phoenix | Braintrust | Langfuse | Humanloop | Logfire |
|---|---|---|---|---|---|---|---|
| 온프레미스/VPC 배포 | [W&B Server](https://weave-docs.wandb.ai/guides/self-hosting) | [LangSmith Self-Hosted](https://docs.smith.langchain.com/self_hosting) | X | [Self-hosting](https://braintrust.dev/docs/admin/self-hosting) | X | [Private Cloud / VPC](https://humanloop.com/docs/security) | [Self-hosted Enterprise](https://pydantic.dev/pricing) |
| Kubernetes 지원 | [K8s Deployment](https://weave-docs.wandb.ai/guides/self-hosting) | [K8s Helm Charts](https://docs.smith.langchain.com/self_hosting) | X | X | X | X | X |
| Docker 지원 | [Docker Deployment](https://weave-docs.wandb.ai/guides/self-hosting) | [Docker Compose](https://docs.smith.langchain.com/self_hosting) | X | X | X | X | X |
| 에어갭(Air-gapped) 환경 지원 | [Air-gapped Support](https://weave-docs.wandb.ai/guides/self-hosting) | [Air-gapped Support](https://docs.smith.langchain.com/self_hosting) | X | X | X | X | X |
| SSO/SAML 연동 | [SSO/SAML](https://weave-docs.wandb.ai/guides/self-hosting) | [SSO Integration](https://docs.smith.langchain.com/administration/identity_management) | X | [SSO/SAML](https://braintrust.dev/docs/admin/access-control) | X | X | X |
| 로컬 실행 | X | X | [Local Launch (px.launch_app)](https://docs.arize.com/phoenix/quickstart) | X | X | X | X |
| Docker 배포 | [W&B Server (Docker)](https://docs.wandb.ai/guides/self-hosted) | X | [Docker Deployment](https://docs.arize.com/phoenix/self-hosting/deploy-with-docker) | X | [Docker Compose](https://langfuse.com/docs/deployment/local) | X | X |
| Kubernetes 배포 | [W&B Server (K8s)](https://docs.wandb.ai/guides/self-hosted) | X | [Kubernetes Deployment (Helm)](https://docs.arize.com/phoenix/self-hosting/deploy-with-helm) | X | Kubernetes (Helm) | X | X |
| 에어갭(Air-gapped) 지원 | [W&B Server Air-gapped](https://docs.wandb.ai/guides/self-hosted) | X | [Self-Hosted Open Source](https://docs.arize.com/phoenix/self-hosting) | X | X | X | X |
| 데이터 보존 정책 제어 | [W&B Server Retention](https://docs.wandb.ai/guides/self-hosted) | X | [Data Retention Settings](https://docs.arize.com/phoenix/settings/data-retention) | X | X | X | X |
| 하이브리드 클라우드 배포 | X | X | X | Hybrid Cloud (Data Plane) | X | X | X |
| 데이터 소유권 보장 | Customer Data Ownership | X | X | [Customer Data Ownership](https://braintrust.dev/docs/start/security) | X | X | X |
| Docker/K8s 지원 | [K8s/Docker](https://docs.wandb.ai/guides/hosting) | X | X | [Docker/K8s Support](https://braintrust.dev/docs/admin/self-hosting) | X | X | Enterprise Deployment |
| 라이선스 | X | X | X | X | [MIT License (Open Source)](https://github.com/langfuse/langfuse/blob/main/LICENSE) | X | X |
| 데이터베이스 요구사항 | [MySQL + Object Store](https://docs.wandb.ai/guides/hosting) | X | X | X | Postgres + ClickHouse | X | X |
| SSO/인증 | [SSO](https://docs.wandb.ai/guides/hosting) | X | X | X | SSO (Enterprise/Pro) | X | X |
| SSO/SAML 지원 | [SSO/SAML](https://wandb.ai/site/enterprise) | X | X | X | X | [SSO](https://humanloop.com/docs/security) | X |
| 데이터 거버넌스 | [Compliance Features](https://wandb.ai/site/security) | X | X | X | X | [SOC2 / GDPR](https://humanloop.com/docs/security) | X |
| SSO/보안 기능 | [SSO / RBAC](https://docs.wandb.ai/guides/hosting) | X | X | X | X | X | [Enterprise Features](https://pydantic.dev/pricing) |
| 에어갭 지원 | [Air-gapped Deployment](https://docs.wandb.ai/guides/hosting) | X | X | X | X | X | X |
| 관리형 서비스 | [W&B Cloud](https://wandb.ai/) | X | X | X | X | X | [Logfire Cloud](https://pydantic.dev/logfire) |


---

## 경쟁사 상세 분석

### LangSmith

**개요**: LangSmith는 LangChain 팀이 개발한 LLM 애플리케이션 수명 주기 관리 플랫폼으로, 프로토타이핑부터 프로덕션 모니터링까지 포괄적인 기능을 제공합니다. 특히 LangChain 및 LangGraph 프레임워크와의 깊은 통합을 통해 에이전트 디버깅과 배포에 강점을 가지며, 최근 AI 디버깅 어시스턴트인 'Polly'와 같은 지능형 기능을 강화하고 있습니다.

**Weave 대비 강점**:
- LangChain/LangGraph 프레임워크와의 완벽한 기본 통합 및 에이전트 상태 시각화
- Annotation Queues를 통한 체계적인 휴먼 피드백(Human-in-the-loop) 워크플로우
- Polly(AI 어시스턴트) 및 Fetch(CLI)와 같은 개발자 생산성 도구
- LangChain Hub와 연동된 방대한 프롬프트 생태계
- 프로덕션 환경에서의 온라인 평가 및 피드백 수집 기능

**Weave 대비 약점**:
- LangChain 생태계 밖의 프레임워크 사용 시 통합 깊이가 상대적으로 얕을 수 있음
- Python 외에 TypeScript/JS를 지원하지만, Weave만큼 다양한 Python ML 라이브러리(DSPy 등)에 대한 광범위한 자동 패칭은 부족할 수 있음
- W&B와 같은 통합 MLOps 플랫폼(실험 관리, 모델 레지스트리 등)의 일부가 아닌 독립적인 LLM 도구로 존재
- 트레이스 양이 많아질 경우 비용이 빠르게 증가할 수 있는 구조

**주요 업데이트**:
- Polly (Beta): 에이전트 디버깅 및 분석을 돕는 AI 어시스턴트 출시 (2025.12)
- LangSmith Fetch: 터미널에서 직접 트레이스를 디버깅할 수 있는 CLI 도구 출시 (2025.12)
- Pairwise Annotation Queues: 두 에이전트 출력을 비교하여 승자를 선택하는 평가 큐 기능 추가 (2025.12)
- Unified Cost Tracking: LLM, 도구, 검색 전반에 걸친 통합 비용 추적 기능 (2025.12)
- Deep Agent Sandboxes: 복잡한 에이전트 코드 실행을 위한 격리된 샌드박스 환경 지원 (2025.11)

| 축 | 판정 | 요약 |
|---|---|---|
| 트레이싱/옵저버빌리티 | 경쟁사 우위 | LangChain 및 LangGraph 기반 애플리케이션에 대한 심층적인 가시성을 제공하며, 토큰 사용량, 비용, 지연 시간을 자동으로 추적합니다. 최근 CLI 도구인 'LangSmith Fetch'와 AI 디버깅 어시스턴트 'Polly'를 도입하여 디버깅 경험을 고도화했습니다. |
| 평가 파이프라인 | 경쟁사 우위 | 데이터셋 기반의 오프라인 평가뿐만 아니라, 프로덕션 로그를 샘플링하여 평가하는 온라인 평가(Online Evaluation) 기능이 강력합니다. 특히 'Annotation Queues'를 통해 사람 또는 LLM이 두 모델의 출력을 비교(Pairwise)하는 워크플로우가 잘 구축되어 있습니다. |
| 데이터셋 관리 | 유사 | 트레이스에서 직접 데이터를 수집하여 데이터셋으로 변환하는 기능이 직관적입니다. CSV/JSON 가져오기 및 내보내기를 지원하며, 평가 및 파인튜닝을 위한 데이터 버전 관리를 제공합니다. |
| 프롬프트 관리 | 경쟁사 우위 | LangChain Hub와 연동되어 공개 프롬프트를 공유하거나 비공개 프롬프트를 버전 관리할 수 있습니다. 플레이그라운드에서 프롬프트를 즉시 테스트하고, 변경 사항을 커밋하여 배포 버전을 관리하는 기능이 강력합니다. |
| 스코어링 | 유사 | LLM-as-a-judge 평가자, 휴리스틱 평가자, 그리고 사용자 정의 Python 함수 평가자를 모두 지원합니다. 특히 어노테이션 큐를 통한 '사람에 의한 평가(Human Feedback)' 기능이 시스템적으로 잘 통합되어 있습니다. |
| LLM/프레임워크 통합 | 유사 | LangChain 및 LangGraph와의 통합은 타의 추종을 불허하며, OpenAI, Anthropic 등 주요 모델 제공자와의 통합도 지원합니다. 최근 Vercel AI SDK, CrewAI 등 외부 프레임워크 지원을 확대하고 있습니다. |
| 가격 | 유사 | 개인 개발자를 위한 무료 티어(월 5,000 트레이스)와 팀을 위한 Plus 티어(인당 $39/월)를 제공합니다. 트레이스 수 기반의 과금 모델을 따르며, 엔터프라이즈는 별도 견적입니다. |
| 셀프호스팅 | 유사 | 데이터 보안 및 규정 준수를 위해 Kubernetes 또는 Docker 기반의 셀프 호스팅(온프레미스/VPC) 옵션을 엔터프라이즈 고객에게 제공합니다. |

---

### Arize Phoenix

**개요**: Arize Phoenix는 오픈소스 기반의 AI 옵저버빌리티 및 평가 플랫폼으로, OpenTelemetry 및 OpenInference 표준을 강력하게 지원합니다. 로컬 환경에서의 즉각적인 실행과 디버깅에 강점이 있으며, Arize AI의 엔터프라이즈 플랫폼과 연동하여 프로덕션 모니터링으로 확장할 수 있습니다.

**Weave 대비 강점**:
- OpenTelemetry 및 OpenInference 표준에 대한 강력한 네이티브 지원 및 생태계 주도
- 로컬 환경(Notebook, Localhost)에서 서버 설치 없이 즉시 실행 가능한 가벼운 아키텍처
- 완전 무료로 사용 가능한 오픈소스 버전 (로컬 데이터 무제한)
- RAG 및 도구 사용(Tool Use)에 특화된 구체적인 평가 메트릭(Faithfulness 등)의 빠른 업데이트

**Weave 대비 약점**:
- W&B와 같은 통합 MLOps 플랫폼(실험 관리, 모델 레지스트리 등)과의 깊은 연동 부재
- 대규모 팀 협업 및 엔터프라이즈 관리 기능은 유료 SaaS(Arize AI)로 넘어가야 함
- Weave의 직관적인 `weave.init()` 및 데코레이터 기반의 단순한 사용성에 비해 OTel 설정이 다소 복잡할 수 있음

**주요 업데이트**:
- v12.35.0 (2026-02-09): Claude Opus 4.6 모델 플레이그라운드 지원 추가
- v2.9.0 (2026-02-02): FaithfulnessEvaluator 추가 및 기존 HallucinationEvaluator 대체 (더 정확한 RAG 평가)
- v2.9.0: 도구 호출 정확도(Tool invocation accuracy) 메트릭 추가
- v12.34.0: tool_selection 평가자 라이브러리 추가

| 축 | 판정 | 요약 |
|---|---|---|
| 트레이싱/옵저버빌리티 | 유사 | OpenTelemetry(OTLP) 기반의 트레이싱을 기본으로 채택하여 표준 호환성이 높으며, LlamaIndex, LangChain, DSPy 등에 대한 자동 계측을 제공합니다. 로컬 노트북 환경에서도 UI를 바로 띄워 트레이스, 검색(Retrieval), 도구 사용을 시각화할 수 있습니다. |
| 평가 파이프라인 | 유사 | 데이터셋에 대해 실험(Experiments)을 실행하고 결과를 비교하는 워크플로우를 제공합니다. Ragas, Deepeval 등 외부 평가 라이브러리와의 통합이 용이하며, 변경 사항에 따른 성능 변화를 추적할 수 있습니다. |
| 데이터셋 관리 | 유사 | 트레이스 데이터를 데이터셋으로 변환하거나 코드/CSV에서 업로드하여 관리할 수 있습니다. 생성된 데이터셋은 실험(Experiments)의 입력으로 사용되거나 파인튜닝을 위해 내보낼 수 있습니다. |
| 프롬프트 관리 | 유사 | 프롬프트 버전을 관리하고, 플레이그라운드에서 다양한 모델과 파라미터로 실험할 수 있습니다. SDK를 통해 코드 내 프롬프트를 동기화하는 기능도 제공합니다. |
| 스코어링 | 유사 | LLM-as-a-judge 기반의 평가자와 코드 기반 메트릭을 제공합니다. 최근 Faithfulness(충실도) 및 도구 호출 정확도(Tool invocation accuracy) 메트릭이 추가되었습니다. |
| LLM/프레임워크 통합 | 유사 | 주요 LLM 프레임워크(LangChain, LlamaIndex, DSPy) 및 모델 제공자(OpenAI, Anthropic, Bedrock)와 폭넓게 통합됩니다. OpenInference 표준을 통해 다양한 라이브러리를 지원합니다. |
| 가격 | 경쟁사 우위 | 완전한 오픈소스 버전(로컬/셀프호스팅)은 무료이며, 관리형 SaaS(Arize AX/Phoenix Cloud)는 무료 티어와 월 $199(Pro) 등의 유료 플랜을 제공합니다. |
| 셀프호스팅 | 경쟁사 우위 | Docker 및 Kubernetes를 통한 셀프호스팅을 공식 지원하며, 로컬 Python 환경에서 `px.launch_app()`으로 즉시 실행 가능한 가벼운 구조를 가집니다. |

---

### Braintrust

**개요**: Braintrust는 엔터프라이즈급 AI 옵저버빌리티 및 평가 플랫폼으로, 'Instrument-Observe-Annotate-Evaluate' 워크플로우를 강조합니다. 특히 자연어 쿼리 기능인 'Loop', 프로덕션 로그에 대한 'Online Scoring', 그리고 데이터 프라이버시를 위한 하이브리드 클라우드 아키텍처(Data Plane 분리)가 특징입니다. 최근 2026년 업데이트를 통해 에이전트 워크플로우(Temporal 통합) 및 IDE(Cursor) 통합을 강화하며 개발자 경험을 확장하고 있습니다.

**Weave 대비 강점**:
- Loop 기능: 자연어 쿼리를 통해 SQL 지식 없이도 복잡한 로그 분석 및 시각화 가능
- Online Scoring: 프로덕션 로그에 대해 UI에서 설정 가능한 자동 채점 규칙 엔진 제공
- 개발 도구 통합: Cursor IDE 및 Temporal 워크플로우 엔진과의 깊은 통합으로 개발자 경험 우수
- Thread View & Kanban: 멀티턴 대화 및 리뷰 프로세스에 최적화된 특화 UI 제공

**Weave 대비 약점**:
- MLOps 생태계: W&B의 방대한 실험 관리 및 모델 레지스트리 생태계와의 연동 부재
- 커뮤니티 규모: W&B 대비 상대적으로 작은 사용자 커뮤니티 및 생태계
- Python 중심 외 지원: Weave가 W&B 생태계를 통해 확보한 다양한 레거시 ML 프레임워크 지원 대비 LLM/Agent에만 집중됨

**주요 업데이트**:
- 2026년 2월: Trace-level scorers 도입 (전체 트레이스 컨텍스트 기반 평가)
- 2026년 2월: Cursor IDE 통합 및 LangSmith 래퍼 출시
- 2026년 1월: Temporal 통합 및 Python/Ruby/Go 자동 인스트루먼테이션 지원
- 2026년 1월: Online Scoring 규칙 생성 기능 및 Kanban 리뷰 레이아웃 추가

| 축 | 판정 | 요약 |
|---|---|---|
| 트레이싱/옵저버빌리티 | 경쟁사 우위 | 실시간 로그 스트리밍과 상세 트레이스 분석을 제공하며, 자연어로 로그를 검색하고 시각화하는 'Loop' 기능이 강력한 차별점입니다. 멀티턴 대화를 위한 Thread View와 리뷰를 위한 Kanban 레이아웃 등 UX가 고도화되어 있습니다. |
| 평가 파이프라인 | 유사 | 코드 기반의 `eval` 함수와 UI 기반의 플레이그라운드를 모두 지원하며, 실험 간 비교 기능이 강력합니다. 데이터셋과 연동된 실험 실행 및 결과 시각화가 체계적입니다. |
| 데이터셋 관리 | 유사 | 데이터셋을 프로젝트의 핵심 구성 요소로 취급하며, UI에서 직접 편집, 버전 관리, CSV/SQL 임포트가 가능합니다. 실험 및 프롬프트 테스트에 즉시 사용할 수 있도록 설계되었습니다. |
| 프롬프트 관리 | 경쟁사 우위 | 프롬프트 엔지니어링을 핵심 워크플로우로 지원하며, Nunjucks 템플릿 문법 지원 및 버전 관리, 플레이그라운드에서의 즉각적인 테스트가 가능합니다. |
| 스코어링 | 경쟁사 우위 | 내장 스코어러 외에 프로덕션 로그에 대해 자동으로 실행되는 'Online Scoring' 규칙과, 에이전트의 전체 실행 흐름을 평가하는 'Trace-level scorers'를 제공합니다. |
| LLM/프레임워크 통합 | 경쟁사 우위 | 주요 LLM 제공자 외에 Temporal, Cursor, Claude Code 등 개발 도구 및 워크플로우 엔진과의 통합이 돋보입니다. LangSmith 래퍼를 통해 경쟁사 도구에서의 마이그레이션도 지원합니다. |
| 가격 | 유사 | 무료 티어로 시작 가능하며, Pro 플랜($249/월)과 엔터프라이즈 플랜을 제공합니다. 투명한 가격 정책을 강조합니다. |
| 셀프호스팅 | 유사 | 엔터프라이즈 고객을 위해 고객의 클라우드 계정 내에 Data Plane을 배포하는 하이브리드 클라우드 모델을 지원하여 데이터 프라이버시를 보장합니다. |

---

### Langfuse

**개요**: Langfuse는 오픈소스(MIT 라이선스) 기반의 LLM 엔지니어링 플랫폼으로, 옵저버빌리티, 평가, 프롬프트 관리, 메트릭 분석을 통합 제공합니다. OpenTelemetry 표준을 적극적으로 채택하여 벤더 종속성을 줄이고, ClickHouse 기반의 고성능 분석 백엔드를 도입하여 대규모 데이터 처리에 강점을 보입니다. 개발자 친화적인 SDK와 'Annotation Queues' 같은 휴먼 피드백 워크플로우가 특징입니다.

**Weave 대비 강점**:
- 완전한 오픈소스(MIT)로 무료 셀프 호스팅 및 커스터마이징 가능
- OpenTelemetry 표준 기반으로 벤더 락인(Lock-in) 최소화 및 타 도구 연동 용이
- 프롬프트 관리를 위한 CMS 기능(배포 라벨, MCP 서버, 웹훅)이 고도화됨
- Annotation Queues를 통한 체계적인 휴먼 피드백/라벨링 워크플로우 제공
- JS/TS SDK 지원으로 프론트엔드 및 Node.js 기반 LLM 앱 추적 용이

**Weave 대비 약점**:
- W&B와 같은 강력한 기존 ML 실험 관리 및 모델 레지스트리 생태계 부재
- Python SDK의 자동 패칭 마법(Magic)이 Weave만큼 광범위하거나 직관적이지 않을 수 있음
- 데이터셋 관리가 UI 중심이며 W&B Artifacts만큼 대규모 바이너리/객체 버전에 최적화되지 않음
- 노트북(Notebook) 환경에서의 인터랙티브한 데이터 탐색 경험이 Weave보다 부족함

**주요 업데이트**:
- 2025-11-20: 프롬프트 관리를 위한 호스팅된 MCP(Model Context Protocol) 서버 출시
- 2025-11-08: 데이터셋 아이템에 대한 JSON Schema 강제 기능 추가
- 2025-11-07: 다중 스코어 비교 및 분석을 위한 Score Analytics 대시보드 출시
- 2025-11-06: 실험 비교 뷰(Compare View)에서 베이스라인 지정 및 휴먼 어노테이션 지원
- 2026-01-14: 트레이스 내에서 정답 데이터를 바로 생성하는 'Corrected Outputs' 기능 추가

| 축 | 판정 | 요약 |
|---|---|---|
| 트레이싱/옵저버빌리티 | 유사 | OpenTelemetry 기반의 트레이싱을 제공하며, LLM 호출뿐만 아니라 비동기 작업, 에이전트 그래프, 세션(Session) 단위의 멀티턴 대화 추적을 지원합니다. 비용 및 지연 시간 분석이 상세하며, 최근 'Corrected Outputs' 기능을 통해 트레이스 내에서 정답 데이터를 바로 구축할 수 있습니다. |
| 평가 파이프라인 | 유사 | 데이터셋 기반의 실험(Experiments) 실행과 LLM-as-a-judge를 지원합니다. 특히 'Annotation Queues' 기능을 통해 사람의 검토 작업을 체계적으로 큐(Queue) 형태로 관리하고 할당할 수 있는 워크플로우가 내장되어 있습니다. |
| 데이터셋 관리 | 유사 | 입출력 쌍을 관리하는 Datasets 기능을 제공하며, 최근 JSON Schema 강제 기능과 폴더 정리 기능이 추가되었습니다. 트레이스에서 관찰된 데이터를 바로 데이터셋으로 추가하는 기능이 편리합니다. |
| 프롬프트 관리 | 경쟁사 우위 | CMS(Content Management System) 스타일의 프롬프트 관리를 제공합니다. 버전 관리, 배포(Labeling), 캐싱뿐만 아니라 MCP(Model Context Protocol) 서버를 지원하여 에이전트가 직접 프롬프트를 가져갈 수 있게 한 점이 특징입니다. |
| 스코어링 | 유사 | 모델 기반 평가(LLM-as-a-judge)와 사용자 피드백, 수동 평가를 모두 'Scores'라는 개념으로 통합 관리합니다. 스코어 분석 대시보드를 통해 평가 지표의 추이를 시각화합니다. |
| LLM/프레임워크 통합 | 유사 | Python 및 JS/TS SDK를 제공하며 OpenTelemetry를 기반으로 하여 확장성이 높습니다. LangChain, LlamaIndex, OpenAI 등 주요 프레임워크와 통합되며, 최근 Bedrock AgentCore, LiveKit 등 다양한 에이전트 프레임워크 지원을 확대하고 있습니다. |
| 가격 | 경쟁사 우위 | 완전한 오픈소스(MIT)로 셀프 호스팅 시 무료로 사용 가능합니다. 클라우드 버전은 월 50k 유닛까지 무료인 Hobby 플랜과 월 $29부터 시작하는 Pro 플랜을 제공합니다. |
| 셀프호스팅 | 경쟁사 우위 | Docker Compose 및 Kubernetes를 통한 간편한 셀프 호스팅을 지원합니다. MIT 라이선스로 제공되어 기업 내부망 구축이 용이하며, 최근 ClickHouse 도입으로 분석 성능이 강화되었습니다. |

---

### Humanloop

**개요**: Humanloop은 프롬프트 엔지니어링과 평가(Evaluation)에 강점을 둔 LLM Ops 플랫폼이었으나, 2025년 8월 Anthropic에 인수됨에 따라 2025년 9월 8일부로 서비스가 종료(Sunset)될 예정입니다. 현재 신규 가입 및 과금이 중단되었으며, 기존 사용자는 데이터를 마이그레이션해야 하는 상태입니다. 과거에는 비기술 직군을 위한 프롬프트 관리 UI와 협업 기능이 핵심 경쟁력이었습니다.

**Weave 대비 강점**:
- 비개발자(PM/도메인 전문가) 친화적인 프롬프트 관리 UI (과거 강점)
- .prompt 파일을 통한 로컬 코드와 웹 UI 간의 원활한 동기화 워크플로우
- Vercel AI SDK와의 긴밀한 통합 (Next.js 생태계)

**Weave 대비 약점**:
- **서비스 종료(Sunset)**: 2025년 9월 8일부로 플랫폼 폐쇄 예정
- 신규 가입 및 과금 중단으로 도입 불가능
- Weave 대비 제한적인 자동 트레이싱(Auto-instrumentation) 프레임워크 지원
- Python 생태계(LlamaIndex, DSPy 등)와의 통합 깊이가 Weave보다 얕음

**주요 업데이트**:
- 2025년 8월: Anthropic에 인수됨
- 2025년 9월 8일: Humanloop 플랫폼 서비스 종료(Sunset) 예정 발표
- 2025년 7월 30일: 모든 과금 중단 및 신규 가입 차단

| 축 | 판정 | 요약 |
|---|---|---|
| 트레이싱/옵저버빌리티 | Weave 우위 | 실시간 로그 모니터링과 사용자 피드백 수집 기능을 제공하여 모델 성능과 비용을 추적했습니다. 그러나 Weave와 같은 심층적인 코드 레벨의 자동 트레이싱보다는 입출력 로깅과 사용자 피드백 연결에 더 중점을 두었습니다. |
| 평가 파이프라인 | Weave 우위 | CI/CD 파이프라인에 통합 가능한 평가 기능과 UI 기반의 정성적 평가 도구를 제공했습니다. 'Evals-driven development'를 강조하며 코드 및 UI에서의 평가 실행을 모두 지원했습니다. |
| 데이터셋 관리 | Weave 우위 | 평가용 데이터셋을 UI에서 생성, 업로드 및 관리하는 기능을 제공했습니다. CSV/JSONL 업로드 및 버전 관리를 지원하여 평가 루프의 핵심 요소로 활용했습니다. |
| 프롬프트 관리 | Weave 우위 | Humanloop의 가장 강력한 기능 중 하나로, `.prompt` 파일 형식을 통한 코드와 UI의 동기화, 비개발자 친화적인 플레이그라운드, 버전 관리 기능을 제공했습니다. |
| 스코어링 | Weave 우위 | LLM-as-a-Judge 방식의 자동 평가와 사람에 의한 피드백(Human Feedback)을 모두 지원했습니다. JSON 스키마 검증이나 특정 룰 기반의 스코어러도 제공했습니다. |
| LLM/프레임워크 통합 | Weave 우위 | 주요 LLM 제공자(OpenAI, Anthropic 등)와 직접 통합을 지원했으며, Vercel AI SDK와의 통합이 강력했습니다. 그러나 Weave만큼 광범위한 프레임워크 자동 패칭을 강조하지는 않았습니다. |
| 가격 | Weave 우위 | 과거에는 Free, Team, Enterprise 티어를 제공했으나, Anthropic 인수 이후 모든 과금이 중단되었으며 신규 구매가 불가능합니다. |
| 셀프호스팅 | Weave 우위 | 엔터프라이즈 고객을 위한 VPC 배포 옵션을 제공했으나, 서비스 종료로 인해 더 이상 지원되지 않거나 유지보수가 불가능합니다. |

---

### Logfire

**개요**: Pydantic 팀이 개발한 Logfire는 OpenTelemetry 기반의 개발자 중심 옵저버빌리티 플랫폼으로, Python 및 Pydantic 생태계와의 강력한 통합을 자랑합니다. SQL을 사용한 트레이스 쿼리 기능과 가벼운 계측(Instrumentation)이 특징이지만, Weave와 달리 체계적인 평가(Evaluation) 파이프라인이나 데이터셋/프롬프트 관리 기능보다는 실시간 애플리케이션 모니터링에 더 집중하고 있습니다.

**Weave 대비 강점**:
- SQL 기반 트레이스 쿼리: 수집된 모든 트레이스 데이터를 SQL로 자유롭게 분석 가능
- Pydantic 생태계 통합: Pydantic 모델 및 PydanticAI와의 네이티브 수준 통합
- 가벼운 계측: `logfire.instrument` 한 줄로 대부분의 Python 라이브러리 자동 추적
- 공격적인 무료 티어: 월 1,000만 스팬 무료 제공 (2026년 기준)

**Weave 대비 약점**:
- 평가 워크플로우 부재: 체계적인 Evaluation/Dataset 객체 및 비교 UI 부족
- 프롬프트 관리 기능 부재: 프롬프트 버전 관리 및 플레이그라운드 UI 없음
- LLM 특화 스코어러 부족: 환각 탐지 등 내장 스코어러가 없어 직접 구현 필요
- 비 Python 생태계 지원 제한: Python/Pydantic 중심이라 타 언어 지원이 상대적으로 약함

**주요 업데이트**:
- v4.22.0 (2026-02): 프로젝트 마이그레이션을 위한 멀티 토큰 지원 추가
- v4.19.0 (2026-01): DSPy 프레임워크 통합 추가
- v3.24.0 (2025-07): LiteLLM 계측(Instrumentation) 지원 추가
- PydanticAI 통합: PydanticAI 에이전트 프레임워크에 대한 네이티브 지원 강화

| 축 | 판정 | 요약 |
|---|---|---|
| 트레이싱/옵저버빌리티 | 유사 | OpenTelemetry를 기반으로 하며 Pydantic 모델 및 Python 함수에 대한 자동 계측이 매우 강력합니다. 특히 수집된 트레이스 데이터를 SQL로 자유롭게 쿼리할 수 있는 기능은 Weave 대비 차별화된 강점입니다. |
| 평가 파이프라인 | Weave 우위 | Logfire는 실시간 모니터링에 집중하며, Weave와 같은 전용 'Evaluation' 객체나 체계적인 데이터셋 기반 평가 워크플로우를 기본 제공하지 않습니다. 주로 트레이스 내 피드백 기록에 의존합니다. |
| 데이터셋 관리 | Weave 우위 | Logfire는 독립적인 데이터셋 버전 관리 기능을 제공하지 않습니다. 데이터는 주로 로그나 스팬의 형태로만 존재합니다. |
| 프롬프트 관리 | Weave 우위 | Logfire는 프롬프트를 트레이스 데이터의 일부로 취급하며, Weave와 같은 독립적인 프롬프트 객체 관리나 플레이그라운드 UI를 제공하지 않습니다. |
| 스코어링 | Weave 우위 | Logfire는 내장된 스코어러 라이브러리가 없으며, 사용자가 직접 구현하여 스팬 속성으로 기록해야 합니다. Weave의 LLM-as-a-judge 같은 구조화된 스코어링 기능이 부족합니다. |
| LLM/프레임워크 통합 | 유사 | Logfire는 PydanticAI, OpenAI, Anthropic, LiteLLM, DSPy 등 최신 프레임워크에 대해 매우 빠르고 광범위한 통합을 제공합니다. 특히 Pydantic 기반 라이브러리와의 호환성이 뛰어납니다. |
| 가격 | 유사 | Logfire는 월 1,000만 스팬이라는 매우 넉넉한 무료 티어를 제공하며, 이후 사용량 기반 과금($2/1M 스팬)을 적용합니다. 개발자 친화적인 투명한 가격 정책을 가지고 있습니다. |
| 셀프호스팅 | 유사 | Logfire는 클라우드 엔터프라이즈 외에도 셀프 호스팅 옵션을 제공합니다. Weave 역시 W&B Server를 통해 온프레미스 배포를 지원합니다. |

---

## 분석 방법론

데이터는 2026-02-11에 Serper.dev 웹 검색, 공식 문서 스크래핑, GitHub/PyPI 피드를 통해 수집되었습니다.
분석은 OpenRouter를 통해 google/gemini-3-pro-preview 모델로 수행되었습니다.

---
layout: default
title: LLM Observability 시장 조사 - 2026-02-13
---

# 주간 LLM Observability 시장 조사 보고서
**날짜**: 2026-02-13 | **모델**: google/gemini-3-pro-preview | **데이터 수집일**: 2026-02-13

## 1. 요약 (Executive Summary)

- LangSmith가 Self-Hosted v0.13을 출시하고 LLM, 도구, 검색 컴포넌트를 모두 아우르는 통합 비용 추적 기능을 도입했습니다.
- Langfuse는 컴플라이언스 모니터링을 위한 Organization Audit Log Viewer를 출시하고, 에이전트 가시성 개선을 위해 reasoning 단계 렌더링 기능을 추가했습니다.
- MLflow는 Tracking Server 내에서 멀티 워크스페이스 관리가 가능하도록 Organization Support를 도입하여, 거버넌스 역량을 전문 LLM 도구 수준으로 끌어올렸습니다.
- Braintrust는 재현 가능한 엔터프라이즈 실험 및 회귀 테스트를 지원하기 위해 Eval용 세밀한 캐시 제어 기능을 추가했습니다.
- Arize Phoenix는 도구 선택 및 성실성(faithfulness) 전용 Eval 도구와 함께, LLM judge 설정을 위한 GUI 기반 위저드(wizard)를 출시했습니다.
- W&B Weave는 멀티모달 입력 Tracing을 기본 지원하는 동시에, 팀 단위의 Eval 비교를 용이하게 하는 동적 리더보드를 출시했습니다.

> **시장 인사이트**: Langfuse와 MLflow가 감사 로그 및 조직 수준 관리 기능을 동시에 출시한 것은 행정적 거버넌스 격차를 좁힘으로써 Weave의 엔터프라이즈 준비성 서사에 도전장을 내민 것입니다. | Arize Phoenix가 LLM judge를 위한 GUI 기반 위저드를 도입한 것은 Weave의 기존 노코드 Eval 생성 워크플로우와 직접 경쟁하며, judge 설정의 사용 편의성을 범용화하고 있습니다. | LangSmith의 심층 에이전트 샌드박스와 Langfuse의 reasoning 단계 렌더링은 현재 복잡한 에이전트 워크플로우를 위한 고수준 실행 그래프가 부족한 Weave의 에이전트 Observability 공백을 부각시킵니다.


## 2. 신규 기능 (최근 30일)

### [W&B Weave](https://app.getbeamer.com/wandb/en)
- **Audio Monitors**: 오디오 출력에 대한 온라인 Eval 모니터로, LLM judge가 텍스트와 함께 MP3/WAV 파일을 평가할 수 있게 합니다. (2026-02-01, Evaluation & Quality)
- **Dynamic Leaderboards**: 필터 및 메트릭에 대한 영구적인 커스터마이징이 가능한 Eval 내 자동 생성 리더보드입니다. (2026-01-29, Evaluation & Quality)
- **Playground 내 커스텀 LoRA**: Weave Playground에서 직접 커스텀 Fine-tuning된 LoRA 가중치를 테스트하고 비교할 수 있도록 지원합니다. (2026-01-16, Development Lifecycle)

### [LangSmith](https://changelog.langchain.com)
- **Trace 미리보기 커스텀**: LangSmith UI에서 Trace 미리보기가 렌더링되는 방식을 커스터마이징하는 기능입니다. (2026-02-06, Integration & DX)
- **LangSmith Self-Hosted v0.13**: 셀프 호스팅 엔터프라이즈 배포 옵션을 위한 신규 버전 릴리스입니다. (2026-01-16, Enterprise & Infrastructure)

### [Langfuse](https://langfuse.com/changelog)
- **Observation 기반 LLM-as-a-Judge**: 개별 Observation에서 직접 LLM-as-a-judge Eval을 실행할 수 있는 지원을 추가했습니다. (2026-02-13, Evaluation & Quality)
- **단일 Observation Eval**: 단일 Observation에 대한 Eval 워크플로우를 활성화했습니다. (2026-02-10, Evaluation & Quality)
- **Thinking/Reasoning 렌더링**: Trace 상세 정보에 생각 및 추론 부분을 위한 렌더링을 추가하여 Chain-of-Thought 가시성을 개선했습니다. (2026-01-28, Core Tracing & Logging)
- **Org Audit Log Viewer**: 조직 수준의 감사 로그 뷰어를 도입했습니다. (2026-01-28, Enterprise & Infrastructure)
- **인라인 Trace 코멘트**: Trace 내 IO 데이터의 일부에 인라인으로 코멘트를 추가할 수 있게 허용했습니다. (2026-01-20, Integration & DX)

### [Braintrust](https://braintrust.dev/docs/changelog)
- **Claude Agent를 위한 서브 에이전트 중첩**: Claude Agent SDK 래퍼에 서브 에이전트 중첩 지원을 추가하여 복잡한 에이전트 워크플로우의 Tracing을 개선했습니다. (2026-02-12, Agent & RAG Specifics)
- **Classifications 필드**: 향상된 메타데이터 태깅 또는 Trace 분류를 위해 SDK에 새로운 'Classifications' 필드를 도입했습니다. (2026-01-31, Core Tracing & Logging)
- **Eval 캐시 제어**: Eval 도중 및 span 엑스포트 후 캐싱을 끌 수 있는 옵션을 추가하여 실험 재현성에 대한 더 많은 제어권을 제공합니다. (2026-01-29, Evaluation & Quality)
- **Python Trace Scoring**: Python에서의 Trace Scoring을 위한 신규 구현체로, 프로그래밍 방식의 Eval 역량을 강화했습니다. (2026-01-21, Evaluation & Quality)
- **Review Span 유형**: SDK에 특정 'review' span 유형을 추가하여 Trace 내 휴먼 리뷰 단계의 분류를 용이하게 했습니다. (2026-01-15, Evaluation & Quality)

### [MLflow](https://mlflow.org/releases)
- **Organization Support**: MLflow Tracking Server에서 멀티 워크스페이스 환경을 지원하여 여러 워크스페이스에 걸친 실험 조직화가 가능해졌습니다. (2026-02-12, Enterprise & Infrastructure)
- **MLflow Assistant**: MLflow UI 내에서 직접 문제를 식별, 진단 및 수정할 수 있도록 돕는 Claude Code 기반의 인프로덕트 챗봇입니다. (2026-01-29, Integration & DX)

### [Arize Phoenix](https://arize.com/docs/phoenix/release-notes)
- **Claude Opus 4.6 지원**: Playground에 Claude Opus 4.6 모델 지원을 추가했습니다. (2026-02-09, Development Lifecycle)
- **도구 선택 Eval 도구**: 라이브러리에 누락되었던 tool_selection Eval 도구를 추가했습니다. (2026-02-06, Evaluation & Quality)
- **성실성(Faithfulness) Eval 도구**: FaithfulnessEvaluator를 도입하고 HallucinationEvaluator를 지원 중단했습니다. (2026-02-02, Evaluation & Quality)
- **도구 호출 정확도 메트릭**: 도구 호출 정확도를 추적하기 위한 새로운 메트릭을 추가했습니다. (2026-01-27, Analytics & Dashboard)
- **메트릭용 Cursor Rule**: 새로운 내장 메트릭(LLM 분류 Eval 도구) 생성을 위한 cursor rule을 추가했습니다. (2026-01-21, Evaluation & Quality)

## 3. 포지셔닝 변화

| 제품 | 현재 | 향후 방향 | 신호 |
|---|---|---|---|
| W&B Weave | 프로그래밍 방식의 Eval 및 실험 추적에 중점을 둔, W&B 에코시스템과 깊게 통합된 개발자 우선 Observability 플랫폼. | 멀티모달 Observability(오디오/비디오)로 확장하고 UI 중심의 Eval 워크플로우를 강화하여 순수 코드를 넘어선 접근성 확대. | 최근 Audio Monitors 및 GUI 기반 Dynamic Leaderboards 출시를 통해 포괄적인 멀티 포맷 Eval 도구로의 확장을 시사. |
| LangSmith | 복잡한 에이전트 동작에 대한 깊은 가시성을 제공하는 LangChain 에코시스템의 주요 Observability 및 엔지니어링 플랫폼. | 향상된 협업, 보안(SSO) 및 비용 관리 기능을 갖춘 포괄적인 엔터프라이즈 LLM 라이프사이클 플랫폼으로 확장. | 통합 비용 추적, Okta SSO 및 심층 에이전트 샌드박스의 최근 추가는 엔터프라이즈급 견고함으로의 전환을 증명. |
| Langfuse | 개발자를 위한 심층 Observability 및 Eval에 집중하는 선도적인 오픈 소스 LLM 엔지니어링 플랫폼. | 대규모 프로덕션 배포를 지원하기 위해 엔터프라이즈 거버넌스 역량과 세밀한 Eval 워크플로우를 확장 중. | 최근 Org Audit Log Viewer 및 세밀한 Observation 수준의 LLM-as-a-Judge 기능 출시는 엔터프라이즈 컴플라이언스 및 정밀 Eval로의 확장을 시사. |
| Braintrust | Eval, 프롬프트 관리, Observability를 단일 개발자 중심 워크플로우로 통합하는 엔터프라이즈급 AI 운영 체제. | 에이전트 워크플로우 및 프로그래밍 방식 Eval과의 통합을 심화하여 더욱 포괄적인 '코드 우선' AI 엔지니어링 환경으로 이동. | 서브 에이전트 중첩, 프로그래밍 방식 Trace Scoring 및 Eval용 세밀한 캐시 제어에 집중한 최근 업데이트. |
| MLflow | 오픈 소스 실험 추적 및 모델 라이프사이클 관리의 업계 표준으로, 현재 GenAI에 맞춰 대폭 조정됨. | 네이티브 프롬프트 관리, Eval 및 디버깅 도구를 추가하여 통합 GenAI 엔지니어링 플랫폼으로 진화. | 최근 릴리스(v3.7-v3.10)가 Trace 비교, 프롬프트 UI, AI 지원 디버깅과 같은 GenAI 기능에 거의 독점적으로 집중. |
| Arize Phoenix | RAG Tracing 및 오프라인 Eval에 집중하는 개발자 중심의 오픈 소스 Observability 플랫폼. | 모델 지원을 확대하는 동시에 도구 사용 및 성실성에 대한 특정 메트릭으로 Eval 역량을 심화. | 최근 릴리스에서 새로운 Eval 도구(Faithfulness, Tool Selection)와 확장된 Playground 모델 지원을 강조. |

## 4. 엔터프라이즈 신호

- Langfuse는 컴플라이언스 및 보안 모니터링을 강화하기 위해 Organization Audit Log Viewer를 출시했습니다.
- MLflow는 Tracking Server 내에서 멀티 워크스페이스 관리가 가능하도록 Organization Support를 도입했습니다.
- LangSmith는 Self-Hosted v0.13을 출시하여 온프레미스 엔터프라이즈 배포에 대한 의지를 공고히 했습니다.
- Braintrust는 재현 가능한 엔터프라이즈 실험을 지원하기 위해 Eval용 세밀한 캐시 제어 기능을 추가했습니다.
- W&B Weave는 팀 기반의 Eval 및 비교 워크플로우를 촉진하기 위해 동적 리더보드를 출시했습니다.

---

## 방법론

데이터는 2026-02-13에 GitHub/PyPI 피드 및 문서 스크래핑을 통해 수집되었습니다.
카테고리 분석은 Perplexity Sonar(웹 검색 + 분석)를 사용하여 수행되었습니다. 종합 분석은 OpenRouter를 통해 google/gemini-3-pro-preview 모델을 사용하여 수행되었습니다.